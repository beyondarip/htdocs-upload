2024-12-23 04:29:57: test
---
2024-12-23 05:14:43: dpkg-query -W -f='${Installed-Size;8}  ${Package}\n' | sort -n
---
2024-12-23 05:21:21: W
ini menggunakan Intel RealSense Depth Camera D455,
dijalankan di jetson nano ubuntu 18.0 
saat di run, camera 1 dan 2 nyala, 
yang kemera 2 benar , tetapi yang kamera 1 aneh warnya nya ungu gitu kenpa itu ?
---
2024-12-23 05:21:28: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading

class RealSenseOpenCVNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        
        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )
        
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message"""
        # Convert BGR ke RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        msg = Image()
        msg.height = rgb_frame.shape[0]
        msg.width = rgb_frame.shape[1]
        msg.encoding = 'rgb8'
        msg.is_bigendian = False
        msg.step = rgb_frame.shape[1] * 3
        msg.data = rgb_frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            # Gunakan GStreamer pipeline untuk RealSense
            gst_str = (
                f"v4l2src device=/dev/video{self.camera_id} ! "
                f"video/x-raw,width={self.width},height={self.height},"
                f"framerate={self.frame_rate}/1 ! "
                "videoconvert ! "
                "video/x-raw,format=BGR ! "
                "appsink max-buffers=1 drop=true"
            )
            
            self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)
            
            if not self.cap.isOpened():
                # Fallback ke direct capture jika GStreamer gagal
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)
                # Set buffer size kecil untuk mengurangi latency
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                
                # Set format pixel
                self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M','J','P','G'))
            
            if not self.cap.isOpened():
                raise Exception(f"Could not open camera {self.camera_id}")
                
            rospy.loginfo(f"Camera {self.camera_id} connected successfully")
            return True
            
        except Exception as e:
            rospy.logerr(f"Failed to connect to camera {self.camera_id}: {str(e)}")
            return False
            
    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    if not ret or frame is None:
                        rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                        # Coba reconnect
                        self.cap.release()
                        self.cap = None
                        continue
                    
                    # Tambahan preprocessing untuk menangani masalah warna
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()
            
    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                
                rospy.loginfo(f"Camera node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start camera node {self.camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error starting camera node: {str(e)}")
            
    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"Camera node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = RealSenseOpenCVNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-23 05:31:21: pip install ultralytics
pip install numpy>=1.18.5
pip install opencv-python>=4.1.2
pip install pillow  # untuk PIL
pip install tk  # untuk tkinter
---
2024-12-23 05:42:28: import cv2
import numpy as np
import time
import tkinter as tk
from tkinter import ttk, Menu
from threading import Thread
from PIL import Image, ImageTk
from ultralytics import YOLO
import os
import sys

# Mute Ultraytics YOLO logging
sys.stdout = open(os.devnull, 'w')
sys.stderr = open(os.devnull, 'w')

class YOLODetection(Thread):
    def __init__(self, model_path="yolov8n.pt"):
        super().__init__()
        self.model = YOLO(model_path)
        self.cap = cv2.VideoCapture(0)
        self.running = False
        self.frame = None

    def run(self):
        self.running = True
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                # Deteksi objek
                results = self.model(frame)
                for result in results:
                    for box in result.boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Koordinat bounding box
                        conf = box.conf[0]  # Confidence score
                        cls = int(box.cls[0])  # Kelas objek
                        label = f"{self.model.names[cls]} {conf:.2f}"
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

                self.frame = frame

    def stop(self):
        self.running = False
        self.cap.release()

    def get_frame(self):
        return self.frame


class Application(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("YOLOv8 Object Detection")
        self.geometry("800x600")

        # Menu
        self.menu_bar = Menu(self)
        self.config(menu=self.menu_bar)

        # Menambahkan menu File
        self.file_menu = Menu(self.menu_bar, tearoff=0)
        self.menu_bar.add_cascade(label="File", menu=self.file_menu)
        self.file_menu.add_command(label="Exit", command=self.quit)

        # Menambahkan menu Edit
        self.edit_menu = Menu(self.menu_bar, tearoff=0)
        self.menu_bar.add_cascade(label="Edit", menu=self.edit_menu)
        self.edit_menu.add_command(label="Preferences")

        # Toolbar
        self.toolbar_frame = tk.Frame(self)
        self.toolbar_frame.pack(side=tk.TOP, fill=tk.X)

        # Tombol-tombol di toolbar
        self.start_button = tk.Button(self.toolbar_frame, text="Start Detection", command=self.start_detection)
        self.start_button.pack(side=tk.LEFT, padx=10)

        self.stop_button = tk.Button(self.toolbar_frame, text="Stop Detection", command=self.stop_detection, state=tk.DISABLED)
        self.stop_button.pack(side=tk.LEFT, padx=10)

        self.draw_button = tk.Button(self.toolbar_frame, text="Start Draw Box", command=self.toggle_draw_box)
        self.draw_button.pack(side=tk.LEFT, padx=10)

        self.close_button = tk.Button(self.toolbar_frame, text="Close", command=self.close_application)
        self.close_button.pack(side=tk.RIGHT, padx=10)

        # Taskbar (Status Bar)
        self.status_bar = tk.Label(self, text="Status: Ready", anchor="w")
        self.status_bar.pack(side=tk.BOTTOM, fill=tk.X)

        # Tampilan video
        self.video_label = tk.Label(self)
        self.video_label.pack()

        # FPS
        self.start_time = time.time()
        self.frame_count = 0
        self.fps = 0

        # Thread deteksi
        self.yolo_thread = None  # Deteksi tidak dimulai sampai tombol start ditekan

        # Update tampilan video
        self.update_video_frame()

        # Status menggambar kotak
        self.drawing_box = False
        self.box_start = None
        self.box_end = None
        self.boxes = []  # Menyimpan kotak yang digambar
        self.current_box = None  # Kotak yang sedang digambar
        self.is_box_drawn = False  # Status apakah kotak sudah digambar

        # Mengikat event mouse
        self.bind_mouse_events()

        # Timer untuk menyimpan gambar crop
        self.last_crop_time = time.time()

    def start_detection(self):
        if self.yolo_thread is None or not self.yolo_thread.is_alive():
            self.yolo_thread = YOLODetection(model_path="yolov8n.pt")
            self.yolo_thread.start()

        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.status_bar.config(text="Status: Detection Started")

        # Reset FPS kalkulasi saat mulai deteksi
        self.frame_count = 0
        self.start_time = time.time()

    def stop_detection(self):
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.status_bar.config(text="Status: Detection Stopped")
        self.yolo_thread.stop()

    def toggle_draw_box(self):
        self.drawing_box = not self.drawing_box
        if self.drawing_box:
            self.draw_button.config(text="Stop Draw Box")
            self.status_bar.config(text="Status: Drawing Box")
        else:
            self.draw_button.config(text="Start Draw Box")
            self.status_bar.config(text="Status: Box Drawing Stopped")

    def update_video_frame(self):
        if self.yolo_thread and self.yolo_thread.frame is not None:
            frame = self.yolo_thread.frame

            # Menghitung FPS hanya jika deteksi aktif
            if self.yolo_thread.running:
                self.frame_count += 1
                elapsed_time = time.time() - self.start_time
                if elapsed_time > 1:
                    self.fps = self.frame_count / elapsed_time
                    self.start_time = time.time()
                    self.frame_count = 0

            # Menampilkan FPS di frame
            cv2.putText(frame, f"FPS: {self.fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            # Menampilkan kotak yang sudah digambar
            for (start, end) in self.boxes:
                cv2.rectangle(frame, start, end, (128, 0, 0), 2)  # Warna maroon (128, 0, 0)

            # Menggambar kotak yang sedang digambar
            if self.drawing_box and self.box_start and self.box_end:
                cv2.rectangle(frame, self.box_start, self.box_end, (128, 0, 0), 2)  # Warna maroon

            # Menampilkan frame yang diubah ke Tkinter
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(frame_rgb)
            image_tk = ImageTk.PhotoImage(image)

            # Update gambar video
            self.video_label.config(image=image_tk)
            self.video_label.image = image_tk

            # Crop gambar jika sudah ada kotak yang digambar dan simpan setiap 5 detik
            if self.boxes:
                current_time = time.time()
                if current_time - self.last_crop_time >= 5:
                    for (start, end) in self.boxes:
                        # Crop gambar di dalam kotak maroon
                        cropped_image = frame[start[1]:end[1], start[0]:end[0]]
                        
                        # Menambahkan kotak deteksi YOLO pada gambar crop
                        results = self.yolo_thread.model(frame)
                        for result in results:
                            for box in result.boxes:
                                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Koordinat bounding box
                                conf = box.conf[0]  # Confidence score
                                cls = int(box.cls[0])  # Kelas objek
                                label = f"{self.yolo_thread.model.names[cls]} {conf:.2f}"
                                cv2.rectangle(cropped_image, (x1 - start[0], y1 - start[1]), (x2 - start[0], y2 - start[1]), (0, 255, 0), 2)
                                cv2.putText(cropped_image, label, (x1 - start[0], y1 - start[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                        
                        # Menyimpan gambar crop
                        cropped_filename = f"cropped_{int(current_time)}.png"
                        cv2.imwrite(cropped_filename, cropped_image)
                    self.last_crop_time = current_time

        # Mengupdate frame video setiap 20 ms
        self.after(20, self.update_video_frame)

    def mouse_press_event(self, event):
        if self.drawing_box and not self.is_box_drawn:
            self.box_start = (event.x, event.y)
            self.box_end = self.box_start

    def mouse_move_event(self, event):
        if self.drawing_box and self.box_start:
            self.box_end = (event.x, event.y)

    def mouse_release_event(self, event):
        if self.drawing_box and self.box_start:
            self.box_end = (event.x, event.y)
            self.boxes.append((self.box_start, self.box_end))  # Menyimpan kotak yang sudah digambar
            self.is_box_drawn = True  # Menandakan bahwa kotak sudah digambar
            self.box_start = None
            self.box_end = None

    def mouse_drag_event(self, event):
        if self.drawing_box and self.box_start:
            # Geser posisi kotak jika masih dalam mode menggambar
            dx = event.x - self.box_start[0]
            dy = event.y - self.box_start[1]
            self.box_start = (self.box_start[0] + dx, self.box_start[1] + dy)
            self.box_end = (self.box_end[0] + dx, self.box_end[1] + dy)

    def bind_mouse_events(self):
        self.bind("<ButtonPress-1>", self.mouse_press_event)
        self.bind("<B1-Motion>", self.mouse_move_event)
        self.bind("<ButtonRelease-1>", self.mouse_release_event)

    def unbind_mouse_events(self):
        self.unbind("<ButtonPress-1>")
        self.unbind("<B1-Motion>")
        self.unbind("<ButtonRelease-1>")

    def close_application(self):
        if self.yolo_thread and self.yolo_thread.is_alive():
            self.yolo_thread.stop()  # Menghentikan thread deteksi jika aktif
        self.quit()  # Menutup aplikasi

if __name__ == "__main__":
    app = Application()
    app.mainloop()
---
2024-12-23 06:11:59: import cv2

cap = cv2.VideoCapture(0)  # Ganti 0 dengan ID kamera Anda
if not cap.isOpened():
    print("Kamera tidak terbuka")
else:
    ret, frame = cap.read()
    if ret:
        cv2.imshow("Test", frame)
        cv2.waitKey(0)
    cap.release()
    cv2.destroyAllWindows()
---
2024-12-23 06:12:24: VIDEOIO(V4L2:/dev/video0): can't open camera by index
[ERROR:0@20.015] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range
---
2024-12-23 06:17:26: import cv2

cap = cv2.VideoCapture(0)  # Ganti 0 jika Anda memiliki lebih dari satu kamera
if not cap.isOpened():
    print("Kamera tidak dapat dibuka")
else:
    while True:
        ret, frame = cap.read()
        if ret:
            cv2.imshow("Kamera", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()
---
2024-12-23 06:57:22: The dbus-python package does not seem to be installed.
  These bindings will be built: QtCore, QtNetwork, QtGui, QtWidgets, QtQml, QtDBus, QtDesigner, QtHelp, QtOpenGL, QtPrintSupport, QtQuick, QtQuickWidgets, QtSql, QtTest, QtWebKit, QtWebKitWidgets, QtXml, Qt, _QOpenGLFunctions_ES2, pylupdate, pyrcc.
  Generating the QtCore bindings...
  /tmp/pip-install-6_cimxa0/pyqt5_c21333293c5a4357b01e3dfac16e9fd7/sip/QtCore/qbytearray.sip: line 118: '%BIGetReadBufferCode' is deprecated and will be removed in SIP v7.0.0
  /tmp/pip-install-6_cimxa0/pyqt5_c21333293c5a4357b01e3dfac16e9fd7/sip/QtCore/qbytearray.sip: line 131: '%BIGetSegCountCode' is deprecated and will be removed in SIP v7.0.0
  /tmp/pip-install-6_cimxa0/pyqt5_c21333293c5a4357b01e3dfac16e9fd7/sip/QtCore/qbytearray.sip: line 138: '%BIGetCharBufferCode' is deprecated and will be removed in SIP v7.0.0
---
2024-12-23 08:14:33: https://github.com/lyuwenyu/RT-DETR/blob/main/rtdetrv2_pytorch/requirements.txt
---
2024-12-23 08:14:41: https://github.com/explainingai-code/FasterRCNN-PyTorch/blob/main/requirements.txt
---
2024-12-23 08:15:52: einops==0.6.1
matplotlib==3.7.2
numpy==1.23.5
opencv_python==4.8.0.74
PyYAML==6.0.1
torch==1.11.0
torchvision==0.12.0
tqdm==4.65.0
---
2024-12-23 08:16:37: torch>=2.0.1
torchvision>=0.15.2
pycocotools
PyYAML
tensorboard
---
2024-12-23 08:22:56: torch>=2.0.1
torchvision>=0.15.2
pycocotools
PyYAML
tensorboard
---
2024-12-23 10:45:50: # Setup repository
sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
sudo apt install curl -y
curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -

# Update package index dan instal ROS Melodic
sudo apt update
sudo apt install ros-melodic-desktop-full -y

# Initialize rosdep
sudo rosdep init
rosdep update

# Source setup file setiap terminal
echo "source /opt/ros/melodic/setup.bash" >> ~/.bashrc
source ~/.bashrc

# Install dependencies untuk menggunakan ROS dengan Python
sudo apt install python-rosdep python-rosinstall python-rosinstall-generator python-wstool build-essential -y
---
2024-12-23 10:45:54: sudo apt install python3-pip -y
pip3 install numpy opencv-python pynmea2
---
2024-12-23 10:45:57: sudo apt install python3-rospy python3-sensor-msgs python3-std-msgs python3-nav-msgs -y
---
2024-12-23 10:46:01: sudo apt install v4l-utils gstreamer1.0-tools gstreamer1.0-plugins-base gstreamer1.0-plugins-good -y
---
2024-12-23 10:46:07: pip3 install opencv-python
---
2024-12-23 10:46:11: mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/
catkin_make
---
2024-12-23 11:16:13: [sudo] password for ris: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package python3-rospy
E: Unable to locate package python3-sensor-msgs
E: Unable to locate package python3-std-msgs
E: Unable to locate package python3-nav-msgs
---
2024-12-23 11:16:56: sudo apt install python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
---
2024-12-23 11:17:14: pip3 install opencv-python
Collecting opencv-python
  Using cached https://files.pythonhosted.org/packages/4a/e7/b70a2d9ab205110d715906fc8ec83fbb00404aeb3a37a0654fdb68eb0c8c/opencv-python-4.10.0.84.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "/tmp/pip-build-y9lr7s5j/opencv-python/setup.py", line 10, in <module>
        from skbuild import cmaker, setup
    ModuleNotFoundError: No module named 'skbuild'
    
    ----------------------------------------
Command "python setup.py egg_info" failed with error code 1 in /tmp/pip-build-y9lr7s5j/opencv-python/
---
2024-12-23 11:19:22: ris@ris-desktop:~/Documents$ sudo apt install python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 python-rospy : Depends: python-roslib but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
ris@ris-desktop:~/Documents$ sudo apt install python-roslib python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 python-roslib : Depends: catkin but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
ris@ris-desktop:~/Documents$ sudo apt install catkin python-roslib python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 catkin : Depends: python-catkin-pkg but it is not going to be installed
 python-catkin-pkg-modules : Conflicts: catkin but 0.7.8-1 is to be installed
E: Error, pkgProblemResolver::Resolve generated breaks, this may be caused by held packages.
ris@ris-desktop:~/Documents$ sudo apt install catkin 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 catkin : Depends: python-catkin-pkg but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
ris@ris-desktop:~/Documents$ sudo apt install python-catkin-pkg 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
python-catkin-pkg is already the newest version (0.5.2-100).
python-catkin-pkg set to manually installed.
The following packages were automatically installed and are no longer required:
  apt-clone archdetect-deb bogl-bterm busybox-static cryptsetup-bin
  dpkg-repack gir1.2-timezonemap-1.0 gir1.2-xkl-1.0 grub-common
  kde-window-manager kinit kio kpackagetool5 kwayland-data kwin-common
  kwin-data kwin-x11 libdebian-installer4 libkdecorations2-5v5
  libkdecorations2private5v5 libkf5activities5 libkf5attica5
  libkf5completion-data libkf5completion5 libkf5declarative-data
  libkf5declarative5 libkf5doctools5 libkf5globalaccel-data libkf5globalaccel5
  libkf5globalaccelprivate5 libkf5idletime5 libkf5jobwidgets-data
  libkf5jobwidgets5 libkf5kcmutils-data libkf5kcmutils5 libkf5kiocore5
  libkf5kiontlm5 libkf5kiowidgets5 libkf5newstuff-data libkf5newstuff5
  libkf5newstuffcore5 libkf5package-data libkf5package5 libkf5plasma5
  libkf5quickaddons5 libkf5solid5 libkf5solid5-data libkf5sonnet5-data
  libkf5sonnetcore5 libkf5sonnetui5 libkf5textwidgets-data libkf5textwidgets5
  libkf5waylandclient5 libkf5waylandserver5 libkf5xmlgui-bin libkf5xmlgui-data
  libkf5xmlgui5 libkscreenlocker5 libkwin4-effect-builtins1 libkwineffects11
  libkwinglutils11 libkwinxrenderutils11 libqgsttools-p1 libqt5multimedia5
  libqt5multimedia5-plugins libqt5multimediaquick-p5 libqt5multimediawidgets5
  libxcb-composite0 libxcb-cursor0 libxcb-damage0 os-prober
  python3-dbus.mainloop.pyqt5 python3-icu python3-pam python3-pyqt5
  python3-pyqt5.qtsvg python3-pyqt5.qtwebkit
  qml-module-org-kde-kquickcontrolsaddons qml-module-qtmultimedia
  qml-module-qtquick2 rdate tasksel tasksel-data
Use 'sudo apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 357 not upgraded.
ris@ris-desktop:~/Documents$ sudo apt install catkin 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 catkin : Depends: python-catkin-pkg but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
ris@ris-desktop:~/Documents$ sudo apt install python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 python-rospy : Depends: python-roslib but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
ris@ris-desktop:~/Documents$ ^C
ris@ris-desktop:~/Documents$ sudo apt install python-rospy python-sensor-msgs python-std-msgs python-nav-msgs -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 python-rospy : Depends: python-roslib but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
---
2024-12-23 11:19:54: Collecting opencv-python
  Using cached https://files.pythonhosted.org/packages/4a/e7/b70a2d9ab205110d715906fc8ec83fbb00404aeb3a37a0654fdb68eb0c8c/opencv-python-4.10.0.84.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
      File "/tmp/pip-build-qyvvrvcx/opencv-python/setup.py", line 10, in <module>
        from skbuild import cmaker, setup
    ModuleNotFoundError: No module named 'skbuild'
    
    ----------------------------------------
Command "python setup.py egg_info" failed with error code 1 in /tmp/pip-build-qyvvrvcx/opencv-python/
---
2024-12-23 17:26:30: sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654
sudo apt update
sudo apt install ros-melodic-desktop-full
---
2024-12-23 17:26:31: pip3 install rospy
pip3 install pyserial
pip3 install pynmea2
pip3 install numpy
pip3 install opencv-python
---
2024-12-23 17:26:54: sudo apt update
sudo apt install -y python3-pip python3-dev build-essential
sudo apt install -y git cmake
---
2024-12-23 17:26:56: sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654
sudo apt update
sudo apt install -y ros-melodic-desktop-full
---
2024-12-23 17:27:01: echo "source /opt/ros/melodic/setup.bash" >> ~/.bashrc
source ~/.bashrc
sudo apt install -y python-rosdep
sudo rosdep init
rosdep update
---
2024-12-23 17:27:04: sudo apt install -y python-catkin-tools
mkdir -p ~/catkin_ws/src
cd ~/catkin_ws/
catkin_make
source devel/setup.bash
---
2024-12-23 17:35:44: setting /run_id to d4f754fe-c14b-11ef-9709-a86e842aa9c6
process[rosout-1]: started with pid [10581]
started core service [/rosout]
process[gps_node-2]: started with pid [10584]
process[camera0_node-3]: started with pid [10585]
process[camera1_node-4]: started with pid [10596]
process[camera2_node-5]: started with pid [10601]
process[recorder_node-6]: started with pid [10602]
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/camera_node.py", line 3, in <module>
    import rospy
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/__init__.py", line 49, in <module>
    from .client import spin, myargv, init_node, \
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/client.py", line 52, in <module>
    import roslib
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/__init__.py", line 50, in <module>
    from roslib.launcher import load_manifest  # noqa: F401
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/launcher.py", line 42, in <module>
    import rospkg
ModuleNotFoundError: No module named 'rospkg'
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/gps_node.py", line 3, in <module>
    import rospy
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/__init__.py", line 49, in <module>
    from .client import spin, myargv, init_node, \
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/client.py", line 52, in <module>
    import roslib
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/__init__.py", line 50, in <module>
    from roslib.launcher import load_manifest  # noqa: F401
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/launcher.py", line 42, in <module>
    import rospkg
ModuleNotFoundError: No module named 'rospkg'
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/camera_node.py", line 3, in <module>
    import rospy
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/__init__.py", line 49, in <module>
    from .client import spin, myargv, init_node, \
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/client.py", line 52, in <module>
    import roslib
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/__init__.py", line 50, in <module>
    from roslib.launcher import load_manifest  # noqa: F401
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/launcher.py", line 42, in <module>
    import rospkg
ModuleNotFoundError: No module named 'rospkg'
[camera2_node-5] process has died [pid 10601, exit code 1, cmd /home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/camera_node.py __name:=camera2_node __log:=/home/ris/.ros/log/d4f754fe-c14b-11ef-9709-a86e842aa9c6/camera2_node-5.log].
log file: /home/ris/.ros/log/d4f754fe-c14b-11ef-9709-a86e842aa9c6/camera2_node-5*.log
[camera2_node-5] restarting process
process[camera2_node-5]: started with pid [10619]
[gps_node-2] process has died [pid 10584, exit code 1, cmd /home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/gps_node.py __name:=gps_node __log:=/home/ris/.ros/log/d4f754fe-c14b-11ef-9709-a86e842aa9c6/gps_node-2.log].
log file: /home/ris/.ros/log/d4f754fe-c14b-11ef-9709-a86e842aa9c6/gps_node-2*.log
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/camera_node.py", line 3, in <module>
    import rospy
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/__init__.py", line 49, in <module>
    from .client import spin, myargv, init_node, \
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/client.py", line 52, in <module>
    import roslib
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/__init__.py", line 50, in <module>
    from roslib.launcher import load_manifest  # noqa: F401
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/launcher.py", line 42, in <module>
    import rospkg
ModuleNotFoundError: No module named 'rospkg'
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/src/gps_camera_monitoring/scripts/recorder_node.py", line 3, in <module>
    import rospy
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/__init__.py", line 49, in <module>
    from .client import spin, myargv, init_node, \
  File "/opt/ros/melodic/lib/python2.7/dist-packages/rospy/client.py", line 52, in <module>
    import roslib
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/__init__.py", line 50, in <module>
    from roslib.launcher import load_manifest  # noqa: F401
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslib/launcher.py", line 42, in <module>
    import rospkg
ModuleNotFoundError: No module named 'rospkg'
---
2024-12-23 17:37:42: # Install ROS Python dependencies
sudo apt install -y python-rospkg
pip3 install rospkg catkin_pkg

# Install tambahan yang mungkin diperlukan
sudo apt install -y python-rospy python3-rospy
---
2024-12-23 17:39:42: <!-- Tambahkan image_view untuk melihat output kamera -->
    <node name="image_view" pkg="image_view" type="image_view" respawn="false" output="screen">
        <remap from="image" to="/camera0/image_raw"/>
        <param name="autosize" value="true" />
    </node>
---
2024-12-23 18:57:17: sudo sh -c "echo '/usr/local/cuda/lib64' >> /etc/ld.so.conf.d/nvidia-tegra.conf“
sudo ldconfig
sudo apt-get install build-essential cmake git unzip pkg-config
sudo apt-get install libjpeg-dev libpng-dev libtiff-dev
sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev
sudo apt-get install libgtk2.0-dev libcanberra-gtk*
sudo apt-get install python3-dev python3-numpy python3-pip
sudo apt-get install libxvidcore-dev libx264-dev libgtk-3-dev
sudo apt-get install libtbb2 libtbb-dev libdc1394-22-dev
sudo apt-get install libv4l-dev v4l-utils
sudo apt-get install libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev
sudo apt-get install libavresample-dev libvorbis-dev libxine2-dev
sudo apt-get install libfaac-dev libmp3lame-dev libtheora-dev
sudo apt-get install libopencore-amrnb-dev libopencore-amrwb-dev
sudo apt-get install libopenblas-dev libatlas-base-dev libblas-dev
sudo apt-get install liblapack-dev libeigen3-dev gfortran
sudo apt-get install libhdf5-dev protobuf-compiler
sudo apt-get install libprotobuf-dev libgoogle-glog-dev libgflags-dev
---
2024-12-23 18:57:34: 
---
2024-12-23 18:57:37: cd ~
wget -O opencv.zip https://github.com/opencv/opencv/archive/4.5.1.zip 
wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.5.1.zip 
unzip opencv.zip 
unzip opencv_contrib.zip
---
2024-12-23 18:57:43: mv opencv-4.5.1 opencv
mv opencv_contrib-4.5.1 opencv_contrib
rm opencv.zip
rm opencv_contrib.zip
---
2024-12-23 18:58:01: cd ~/opencv
mkdir build
cd build
---
2024-12-23 18:58:07: cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules -D EIGEN_INCLUDE_PATH=/usr/include/eigen3 -D WITH_OPENCL=OFF -D WITH_CUDA=ON -D CUDA_ARCH_BIN=5.3 -D CUDA_ARCH_PTX="" -D WITH_CUDNN=ON -D WITH_CUBLAS=ON -D ENABLE_FAST_MATH=ON -D CUDA_FAST_MATH=ON -D OPENCV_DNN_CUDA=ON -D ENABLE_NEON=ON -D WITH_QT=OFF -D WITH_OPENMP=ON -D WITH_OPENGL=ON -D BUILD_TIFF=ON -D WITH_FFMPEG=ON -D WITH_GSTREAMER=ON -D WITH_TBB=ON -D BUILD_TBB=ON -D BUILD_TESTS=OFF -D WITH_EIGEN=ON -D WITH_V4L=ON -D WITH_LIBV4L=ON -D OPENCV_ENABLE_NONFREE=ON -D INSTALL_C_EXAMPLES=OFF -D INSTALL_PYTHON_EXAMPLES=OFF -D BUILD_NEW_PYTHON_SUPPORT=ON -D BUILD_opencv_python3=TRUE -D OPENCV_GENERATE_PKGCONFIG=ON -D BUILD_EXAMPLES=OFF ..
---
2024-12-23 19:25:54: sudo fallocate -l 4G /var/swapfile 
sudo chmod 600 /var/swapfile
 sudo mkswap /var/swapfile
 sudo swapon /var/swapfile
 sudo bash -c 'echo "/var/swapfile swap swap defaults 0 0"  >> /etc/fstab’
---
2024-12-23 19:33:40: lfcd () {
        tmp="$(mktemp)"
        ~/lf-hp -last-dir-path="$tmp" "$@"
        if [ -f "$tmp" ]
        then
                dir="$(cat "$tmp")"
                rm -f "$tmp" > /dev/null
                [ -d "$dir" ] && [ "$dir" != "$(pwd)" ] && cd "$dir"
        fi
}
---
2024-12-23 19:34:00: lfcd () {
        tmp="$(mktemp)"
        ~/lf -last-dir-path="$tmp" "$@"
        if [ -f "$tmp" ]
        then
                dir="$(cat "$tmp")"
                rm -f "$tmp" > /dev/null
                [ -d "$dir" ] && [ "$dir" != "$(pwd)" ] && cd "$dir"
        fi
}
---
2024-12-23 19:35:10: bindkey -s '^[E' 'lfcd\n'

alias .="cd .."            # Up one level
alias ..="cd ../.."        # Up two levels
alias ...="cd ../../.."    # Up three levels
alias c="clear"
alias df='df -H'
alias du='du -ch'
# alias lu="du -ha --max-depth=1"
alias lu="du -ha --max-depth=1 | sort --human-numeric-sort "
# alias pwdc="pwd | xclip -selection clipboard"
alias history="fc -l 1"
alias o="xdg-open"
alias mv="mv -iv" #  if duplicate or exist must ask first
alias cp="cp -ivr" #  if duplicate or exist must ask first

alias v="vim"
alias b="cd -"              # Return to previous directory
alias d="cd ~/.dotfiles"
alias h="cd ~"              # Home directory
alias l="ls -Fh --color --group-directories-first"
alias la="ls -AFh --color --group-directories-first"
alias ll="ls -AFhlo --color --group-directories-first"
alias grep='grep --color=auto'
alias fgrep='fgrep --color=auto'
alias egrep='egrep --color=auto'
---
2024-12-23 19:40:16: set-option -g prefix C-a
unbind C-b
unbind l
unbind c
unbind %
unbind n
unbind p
unbind '"'
unbind "'"
bind C-a send-prefix

set-option -g default-terminal "screen-256color"

setw -g mode-keys vi
set-window-option -g mode-keys vi

# resize panes more easily
bind < resize-pane -L 10
bind > resize-pane -R 10
bind - resize-pane -D 10
bind + resize-pane -U 10

# Use v to trigger selection
bind-key -T copy-mode-vi v send-keys -X begin-selection
bind-key -T copy-mode-vi V send-keys -X rectangle-toggle
# Use y to yank current selection
bind-key -T copy-mode-vi y send-keys -X copy-pipe-and-cancel pbcopy

bind -r ^ last-window
bind k select-pane -U
bind j select-pane -D
bind h select-pane -L
bind l select-pane -R

# change pane switching without prefix
bind -n C-S-Left  select-pane -L # bind -n M-S-Up   resize-pane -U 2
bind -n C-S-Right select-pane -R # bind -n M-S-Down resize-pane -D 4
bind -n C-S-Up    select-pane -U # bind -n M-S-Up   resize-pane -U 2
bind -n C-S-Down  select-pane -D # bind -n M-S-Down resize-pane -D 4

bind -n C-M-h select-pane -L # bind -n M-S-Up   resize-pane -U 2
bind -n C-M-l select-pane -R # bind -n M-S-Down resize-pane -D 4
bind -n C-M-j select-pane -U # bind -n M-S-Up   resize-pane -U 2
bind -n C-M-k select-pane -D # bind -n M-S-Down resize-pane -D 4

# reaload config
bind r source-file ~/.config/tmux/tmux.conf \; display-message " Config successfully reloaded!"

# switching window
# bind -n M-Left previous-window
# bind -n M-Right next-window
bind -n M-";" prev
bind -n M-"'" next
bind -n M-left switch-client -n
bind -n M-right switch-client -p
# bind -n M-";" switch-client -n # bind -n M-S-Up   resize-pane -U 2
# bind -n M-"'" switch-client -p # bind -n M-S-Down resize-pane -D 4
#bind -r j switch-client -n
#bind -r k switch-client -p

# spliting window keys
bind-key "'"   split-window -c '#{pane_current_path}' -h # bind v split-window -h
bind-key '"'   split-window -c '#{pane_current_path}' -v # bind h split-window -v

# new tab
bind -n C-t    new-window -c '#{pane_current_path}' -a
bind c         new-window -c '#{pane_current_path}' -a -d
bind -n C-S-T  new-window -c '~'

# resize pane
# bind -n C-S-M-Left  resize-pane -L 2 # bind -n M-S-Up resize-pane -U 2
# bind -n C-S-M-Right resize-pane -R 2 # bind -n M-S-Down resize-pane -D 4
# bind -n C-S-M-Up    resize-pane -U 2 # bind -n M-S-Up resize-pane -U 2
# bind -n C-S-M-Down  resize-pane -D 2 # bind -n M-S-Down resize-pane -D 4

# config regular
set -g base-index 1

#-------- Mouse {{{
#------------------------------------------------------
# mouse to highlight and copy; to paste use "prefix + ]"
# http://awhan.wordpress.com/2012/04/18/tmux-copy-paste-with-mouse/

# temporarily allow regular terminal copy mode while mouse mode is enable
# visual line:  shift+mouse
# visual block: ctrl+shift+mouse <-- only works on some terminals
# visual block: ctrl+mouse <-- (while mouse mode is disable) only works on some terminals

# toggle mouse mode (tmux 2.1 or higher)
# https://www.reddit.com/r/tmux/comments/3paqoi/tmux_21_has_been_released/cw552qd
bind-key m set-option -g -q mouse on \; display-message 'Mouse: ON'
bind-key M set-option -g -q mouse off \; display-message 'Mouse: OFF'
setw -g mouse on

# bind-key m \
#   set-option -g -q mouse on \;\
#   display-message 'Mouse: ON'
#
# bind-key M \
#   set-option -g -q mouse off \;\
#   display-message 'Mouse: OFF'

# toggle mouse mode (old)
# http://tangledhelix.com/blog/2012/07/16/tmux-and-mouse-mode/
# bind-key m \
#   set-option -g mode-mouse on \;\
#   set-option -g mouse-resize-pane on \;\
#   set-option -g mouse-select-pane on \;\
#   set-option -g mouse-select-window on \;\
#   display-message 'Mouse: ON'
# bind-key M \
#   set-option -g mode-mouse off \;\
#   set-option -g mouse-resize-pane off \;\
#   set-option -g mouse-select-pane off \;\
#   set-option -g mouse-select-window off \;\
#   display-message 'Mouse: OFF'

# enable PageUp/Down as copy mode
# if in ncurses app (vim,nano ..etc) then functions as normal
bind-key -T root PPage if-shell -F "#{alternate_on}" "send-keys PPage" "copy-mode -e; send-keys PPage"
# bind-key -T copy-mode-vi PPage page-up
# bind-key -T copy-mode-vi NPage page-down

# bind-key -t vi-copy PPage page-up
# bind-key -t vi-copy NPage page-down

# bind-key -T root WheelUpPane if-shell -F -t = "#{alternate_on}" "send-keys -M" "select-pane -t =; copy-mode -e; send-keys -M"
# bind-key -T root WheelDownPane if-shell -F -t = "#{alternate_on}" "send-keys -M" "select-pane -t =; send-keys -M"
# bind-key -t vi-copy WheelUpPane halfpage-up
# bind-key -t vi-copy WheelDownPane halfpage-down

# demo: https://www.youtube.com/watch?v=N0RL_J0LT9A
# source: https://github.com/wincent/wincent/blob/aa3a322e3a911dabe0ef398ebb1fd7c77209b8ac/roles/dotfiles/files/.tmux.conf
# Mouse can be used to select panes, select windows (by clicking on the status
# bar), resize panes. For default bindings see `tmux list-keys` and `tmux
# list-keys -t vi-copy`.
# set -g mouse on

# Restore pre-2.1 behavior of scrolling with the scrollwheel in Vim, less, copy
# mode etc, otherwise entering copy mode if not already in it.
#
#   if in copy mode (pane_in_mode) || using the mouse already (mouse_any_flag)
#     pass through mouse events to current pane (send -Mt=)
#   elsif in alternate screen mode
#     send `Up` key
#   else
#     enter copy mode (-e exits if we scroll to the bottom)
#   end
#
bind-key -T root WheelUpPane \
  if-shell -Ft= '#{?pane_in_mode,1,#{mouse_any_flag}}' \
    'send -Mt=' \
    'if-shell -Ft= "#{alternate_on}" "send -t= Up" "copy-mode -et="'
bind-key -T root WheelDownPane \
  if-shell -Ft = '#{?pane_in_mode,1,#{mouse_any_flag}}' \
    'send -Mt=' \
    'if-shell -Ft= "#{alternate_on}"  "send -t= Down" "send -Mt="'

# }}}

# Window options
setw -g mode-keys vi
setw -g monitor-activity on
setw -g aggressive-resize on

# history
set -g history-limit 10000
set -sg escape-time 0
# set display timelimit
set-option -g display-panes-time 2000
set-option -g display-time 1000
#set -sg repeat-time

# enable shell hotkeys C-left/C-right
# http://sigizmund.com/tmux-and-c-leftc-right/
set-window-option -g xterm-keys on

# vim keys in command prompt
set-option -g status-keys vi

# history size
set-option -g history-limit 20000

# kill window/pane without confirmation
# http://unix.stackexchange.com/a/30283
bind-key & kill-session
bind-key x kill-pane

# copying selection vim style
# http://jasonwryan.com/blog/2011/06/07/copy-and-paste-in-tmux/
# https://github.com/myfreeweb/dotfiles/blob/master/tmux.conf
bind-key Escape copy-mode			# default key: prefix [
bind-key k copy-mode			    # default key: prefix [
bind-key j copy-mode			    # default key: prefix [
bind-key p paste-buffer				# default key: prefix ] ; interactively clipboard history key: prefix =
bind-key + delete-buffer \; display-message "Deleted current Tmux Clipboard History"

# set the current tmux version (use this variable on if-shell commands)
# run-shell "tmux set-environment -g TMUX_VERSION $(tmux -V | cut -c 6-)"
run-shell "tmux set-environment -g TMUX_VERSION $(tmux -V | cut -c 6- | tr -d '[A-Z][a-z]')" # remove alphabets e.g 2.9a ==> 2.9 only

# vim copy mode rebinds for (tmux 2.4+)
# https://shapeshed.com/custom-vim-bindings-in-tmux-2-4/
# https://github.com/tmux/tmux/issues/754#issuecomment-303156000
# https://stackoverflow.com/a/40902312
# Note: rectangle-toggle (aka Visual Block Mode) > hit v then C-v to trigger it
if-shell -b '[ "$(echo "$TMUX_VERSION >= 2.4" | bc)" = 1 ]' \
  'bind-key -T copy-mode-vi v send-keys -X begin-selection; \
  bind-key -T copy-mode-vi V send-keys -X select-line; \
  bind-key -T copy-mode-vi C-v send-keys -X rectangle-toggle; \
  bind-key -T choice-mode-vi h send-keys -X tree-collapse ; \
  bind-key -T choice-mode-vi l send-keys -X tree-expand ; \
  bind-key -T choice-mode-vi H send-keys -X tree-collapse-all ; \
  bind-key -T choice-mode-vi L send-keys -X tree-expand-all ; \
  bind-key -T copy-mode-vi MouseDragEnd1Pane send-keys -X copy-pipe "xclip -in -selection clipboard"; \
  bind-key -T copy-mode-vi y send-keys -X copy-pipe "xclip -in -selection clipboard"'

# use vim as copy mode
bind-key v capture-pane \; save-buffer /tmp/tmux-buffer \; new-window -n "vim-copymode" '$SHELL -c "vim /tmp/tmux-buffer"'

bind-key -T root PPage if-shell -F "#{alternate_on}" "send-keys PPage" "copy-mode -e; send-keys PPage"

# scoll down / up with mouse
bind-key -T root WheelUpPane \
  if-shell -Ft= '#{?pane_in_mode,1,#{mouse_any_flag}}' \
    'send -Mt=' \
    'if-shell -Ft= "#{alternate_on}" "send -t= Up" "copy-mode -et="'
bind-key -T root WheelDownPane \
  if-shell -Ft = '#{?pane_in_mode,1,#{mouse_any_flag}}' \
    'send -Mt=' \
    'if-shell -Ft= "#{alternate_on}"  "send -t= Down" "send -Mt="'




set -ga terminal-overrides ",xterm-256color*:Tc"

# Border Style
set -g pane-border-style "fg=#444b6a"
set -g pane-active-border-style "fg=#444b6a"

# Message line
set -g message-style 'bg=colour235 fg=colour254'
## set -g message-style "fg=#1a1b26,bg=#ff7a93,bold" # orange

# Window status
# setw -g window-status-style 'bg=colour237 fg=colour247'
# setw -g window-status-current-style 'bg=colour239 fg=colour220'
# setw -g window-status-current-format ' #I:#W #{?window_zoomed_flag,(zoomed) ,}'

# Window Section
setw -g window-style "fg=#787c99"
setw -g window-active-style "fg=#a9b1d6"
setw -g window-status-separator ''
## setw -g window-status-format "#[fg=#414868,bg=#1f2335]"

# window status
# set -g status-style 'bg=#333333 fg=#5eacd3'
set -g status-style 'bg=#1f2335 fg=#5eacd3'
setw -g window-status-format ' #I:#W#{?#{>:#{window_panes},1},(#{window_panes}),} '
#================
# setw -g window-status-style 'bg=colour237 fg=colour247'
# setw -g window-status-style 'bg=#1f2335 fg=colour247' # OR
setw -g window-status-style 'bg=#1f2335 fg=colour247' # OR
#================
set -g window-status-current-format "\
#{?client_prefix,#[fg=#1a1b26]#[bg=#f7768e],\
#{?window_zoomed_flag,#[fg=#1a1b26]#[bg=#0db9d7],\
#{?pane_synchronized,#[fg=#1a1b26]#[bg=#b9f27c],\
#{?window_marked_flag,#[fg=#1a1b26]#[bg=#ff9e64],\
#[fg=#1a1b26]#[bg=#CCA6C9]}}}}"
#================
# bg=#CCA6C9 for pink backgound
# bg=#7aa2f7 for blue backgound
#================
setw -g window-status-current-style 'bg=#1f2335 fg=#7aa2f7'
setw -ga window-status-current-format "  #[bold]#I:#W  "

## Left Section
# set -g status-left "#[fg=#23262E,bg=#7aa2f7,bold] #S|#[fg=#7aa2f7,bg=#1f2335,nobold,nounderscore,noitalics]"
set -g status-left "#[fg=#7aa2f7,bg=#1f2335,bold] #S|#[fg=#7aa2f7,bg=#1f2335,nobold,nounderscore,noitalics]"

# set -g status-left "#[fg=#1a1b26,bg=#7aa2f7]"
# set -ga status-left "\
# #{?client_prefix,#[bg=#f7768e]  \uf794,\
# #{?window_zoomed_flag,#[bg=#0db9d7]  \uf793,\
# #{?pane_synchronized,#[bg=#b9f27c]  \uf46a,\
# #{?window_marked_flag,#[bg=#ff9e64]  \uf5c0,  \uf792}}}}  #[bg=#1a1b26] "

## Right Section
set -g status-right-length 150
setw -g status-right "\
#{?client_prefix,#[fg=#1a1b26]#[bg=#f7768e],\
#{?window_zoomed_flag,#[fg=#1a1b26]#[bg=#0db9d7],\
#{?pane_synchronized,#[fg=#1a1b26]#[bg=#b9f27c],\
#{?window_marked_flag,#[fg=#1a1b26]#[bg=#ff9e64],\
#[fg=#7aa2f7]#[bg=#1f2335]}}}}"
# setw -ga status-right " #{?#{==:#{pane_current_path},#{HOME}},#[bold]~,\uf74a #[bold]#{b:pane_current_path}},%d %b %R"
setw -ga status-right " #{?#{==:#{pane_current_path},#{HOME}},#[bold]~,\uf74a #[bold]#{b:pane_current_path}}"
---
2024-12-23 20:25:14: https://raspberry-valley.azurewebsites.net/NVIDIA-Jetson-Nano/
---
2024-12-24 06:10:20: rg/packages/6e/c2/61d3e0f47e2b74ef40a68b9e6ad5984f6241a942f7cd3bbfbdbd03861ea9/tomli-2.2.1-py3-none-any.whl.metadata
    Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)
  Downloading sip-6.9.1-py3-none-any.whl (471 kB)
  Downloading PyQt_builder-1.17.0-py3-none-any.whl (3.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/3.7 MB 1.8 MB/s eta 0:00:00
  Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 1.6 MB/s eta 0:00:00
  Downloading packaging-24.2-py3-none-any.whl (65 kB)
  Downloading tomli-2.2.1-py3-none-any.whl (14 kB)
  Installing collected packages: tomli, setuptools, packaging, sip, PyQt-builder
    Creating /tmp/pip-build-env-i2lu_9zd/overlay/bin
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-build to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-distinfo to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-install to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-module to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-sdist to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/sip-wheel to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/pyqt-bundle to 775
    changing mode of /tmp/pip-build-env-i2lu_9zd/overlay/bin/pyqt-qt-wheel to 775
  Successfully installed PyQt-builder-1.17.0 packaging-24.2 setuptools-75.6.0 sip-6.9.1 tomli-2.2.1
  Installing build dependencies ... done
  Running command Getting requirements to build wheel
  Getting requirements to build wheel ... done
  Running command Preparing metadata (pyproject.toml)
  Querying qmake about your Qt installation...
  Traceback (most recent call last):
    File "/home/ris/gps_camera_ws/env39_2/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 353, in <module>
      main()
    File "/home/ris/gps_camera_ws/env39_2/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 335, in main
      json_out['return_val'] = hook(**hook_input['kwargs'])
    File "/home/ris/gps_camera_ws/env39_2/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 152, in prepare_metadata_for_build_wheel
      whl_basename = backend.build_wheel(metadata_directory, config_settings)
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/sipbuild/api.py", line 28, in build_wheel
      project = AbstractProject.bootstrap('wheel',
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/sipbuild/abstract_project.py", line 74, in bootstrap
      project.setup(pyproject, tool, tool_description)
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/sipbuild/project.py", line 608, in setup
      self.apply_user_defaults(tool)
    File "project.py", line 68, in apply_user_defaults
      super().apply_user_defaults(tool)
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/pyqtbuild/project.py", line 51, in apply_user_defaults
      super().apply_user_defaults(tool)
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/sipbuild/project.py", line 237, in apply_user_defaults
      self.builder.apply_user_defaults(tool)
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/pyqtbuild/builder.py", line 58, in apply_user_defaults
      self._get_qt_configuration()
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/pyqtbuild/builder.py", line 486, in _get_qt_configuration
      for line in project.read_command_pipe([self.qmake, '-query']):
    File "/tmp/pip-build-env-i2lu_9zd/overlay/lib/python3.9/site-packages/sipbuild/project.py", line 575, in read_command_pipe
      raise UserException(
  sipbuild.exceptions.UserException
  error: subprocess-exited-with-error

  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> See above for output.

  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /home/ris/gps_camera_ws/env39_2/bin/python /home/ris/gps_camera_ws/env39_2/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpqqystqr2
  cwd: /tmp/pip-install-22wmg9gl/pyqt5_5cfd010733a24592a9441ab6dad86707
  Preparing metadata (pyproject.toml) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
---
2024-12-24 10:04:11: <launch>
    <node name="camera_node" pkg="your_package_name" type="camera_node.py" output="screen">
        <param name="video_source" value="video"/>
        <param name="video_path" value="$(find your_package_name)/media/videos/your_video.mp4"/>
    </node>
</launch>
---
2024-12-24 10:12:44: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        # Parameter dasar
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        
        # Parameter baru untuk video lokal
        self.use_local_video = rospy.get_param('~use_local_video', False)  # Flag untuk switch mode
        self.video_path = rospy.get_param('~video_path', '')  # Path ke video lokal
        self.loop_video = rospy.get_param('~loop_video', True)  # Option untuk loop video
        
        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )
        
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message tanpa cv_bridge"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_local_video:
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Connected to local video: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)
                
                if not self.cap.isOpened():
                    gst_str = (
                        f"v4l2src device=/dev/video{self.camera_id} ! "
                        f"video/x-raw, width={self.width}, height={self.height} ! "
                        "videoconvert ! video/x-raw, format=BGR ! "
                        "appsink"
                    )
                    self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)
                    
            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_local_video else f'camera {self.camera_id}'}")
                    
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera {self.camera_id}'} connected successfully")
            return True
                
        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_local_video else f'camera {self.camera_id}'}: {str(e)}")
            return False
            
    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    # Handle video looping untuk local video
                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset video ke awal
                        ret, frame = self.cap.read()
                    
                    if not ret:
                        if self.use_local_video:
                            rospy.loginfo("End of video reached")
                            if not self.loop_video:
                                self.running = False
                            continue
                        else:
                            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                            continue
                    
                    # Resize frame jika diperlukan
                    if frame.shape[1] != self.width or frame.shape[0] != self.height:
                        frame = cv2.resize(frame, (self.width, self.height))
                    
                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()
            
    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                
                rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start {'video' if self.use_local_video else f'camera'} node {self.camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error starting {'video' if self.use_local_video else f'camera'} node: {str(e)}")
            
    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-24 10:16:05: cameras:
  frame_rate: 30
  resolution:
    width: 640
    height: 480
  buffer_size: 1
  retry_interval: 1.0  # seconds
  # Tambahkan parameter untuk video lokal
  local_video:
    enabled: false  # Set true untuk menggunakan video lokal
    path: ""        # Path ke video lokal Anda
    loop: true     # Set true untuk memutar ulang video
---
2024-12-24 10:16:10: <!-- Camera Node dengan parameter video lokal -->
<node name="camera0_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
    <param name="camera_id" value="0"/>
    <param name="use_local_video" value="false"/>
    <param name="video_path" value="$(find gps_camera_monitoring)/videos/your_video.mp4"/>
    <param name="loop_video" value="true"/>
</node>
---
2024-12-24 10:26:18: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image, NavSatFix
from std_srvs.srv import SetBool, SetBoolResponse
import os
from datetime import datetime
import threading

class RecorderNode:
    def __init__(self):
        rospy.init_node('recorder_node', anonymous=True)
        
        # Setup paths
        self.save_path = rospy.get_param('~save_path', '~/recorded_data')
        self.save_path = os.path.expanduser(self.save_path)
        os.makedirs(self.save_path, exist_ok=True)

        # Parameter baru untuk source type tracking
        self.camera_sources = {
            0: None,  # Will store 'webcam' or 'video'
            1: None,
            2: None
        }

        self.record_interval = rospy.get_param('/recorder/record_interval', 1.0)
        rospy.loginfo(f"Recording interval set to {self.record_interval} seconds")
        
        # Status recording
        self.is_recording = False
        self.current_session = None
        self.lock = threading.Lock()
        
        # Latest data holders
        self.latest_gps = None
        self.latest_images = {
            0: None,
            1: None,
            2: None
        }
        
        # Subscribe ke GPS
        rospy.Subscriber('/gps/fix', NavSatFix, self.gps_callback)
        
        # Subscribe ke camera parameters untuk mendeteksi source type
        for cam_id in range(3):
            try:
                use_local_video = rospy.get_param(f'/camera{cam_id}_node/use_local_video', False)
                self.camera_sources[cam_id] = 'video' if use_local_video else 'webcam'
                rospy.loginfo(f"Camera {cam_id} source: {self.camera_sources[cam_id]}")
            except Exception as e:
                rospy.logwarn(f"Could not determine source type for camera {cam_id}: {str(e)}")
        
        # Subscribe ke 3 kamera dengan enhanced callback
        for cam_id in range(3):
            rospy.Subscriber(
                f'/camera{cam_id}/image_raw',
                Image,
                lambda msg, cid=cam_id: self.camera_callback(msg, cid)
            )
        
        # Timer untuk recording periodic
        self.record_timer = None
        
        # Service untuk control recording
        self.record_service = rospy.Service(
            'recorder/set_recording',
            SetBool,
            self.handle_recording_request
        )
        
        rospy.loginfo("Enhanced Recorder node initialized")

    def create_session(self):
        """Buat session recording baru dengan informasi source"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_path = os.path.join(self.save_path, timestamp)
        os.makedirs(session_path, exist_ok=True)
        os.makedirs(os.path.join(session_path, "images"), exist_ok=True)
        
        # Create metadata file untuk tracking source type
        metadata_file = open(os.path.join(session_path, "session_metadata.txt"), "w")
        metadata_file.write("Recording Session Configuration:\n")
        for cam_id, source in self.camera_sources.items():
            metadata_file.write(f"Camera {cam_id}: {source}\n")
        metadata_file.close()
        
        data_file = open(os.path.join(session_path, "gps_data.csv"), "w")
        data_file.write("timestamp,latitude,longitude,camera0_image,camera1_image,camera2_image\n")
        
        rospy.loginfo(f"Created new recording session at: {session_path}")
        return {
            'path': session_path,
            'data_file': data_file,
            'timestamp': timestamp
        }

    def camera_callback(self, msg, camera_id):
        """Enhanced camera callback dengan frame validation"""
        try:
            with self.lock:
                # Basic validation
                if msg.width == 0 or msg.height == 0:
                    rospy.logwarn(f"Received invalid frame from camera {camera_id}")
                    return
                
                self.latest_images[camera_id] = msg
                # rospy.loginfo(f"Valid frame received from camera {camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error in camera {camera_id} callback: {str(e)}")

    def record_data(self, event=None):
        """Enhanced record data dengan better error handling"""
        if not self.is_recording:
            return
            
        try:
            with self.lock:
                missing_data = []
                if self.latest_gps is None:
                    missing_data.append("GPS")
                
                # Check untuk missing camera data
                for cam_id in range(3):
                    if self.latest_images[cam_id] is None:
                        missing_data.append(f"Camera{cam_id}")
                
                if missing_data:
                    rospy.logwarn(f"Missing data from: {', '.join(missing_data)}")
                    return
                    
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                image_filenames = {}
                
                # Process setiap kamera dengan enhanced error handling
                for camera_id in range(3):
                    if self.latest_images[camera_id] is not None:
                        try:
                            # Convert image
                            image = np.frombuffer(self.latest_images[camera_id].data, 
                                                dtype=np.uint8)
                            image = image.reshape(
                                self.latest_images[camera_id].height,
                                self.latest_images[camera_id].width,
                                -1
                            )
                            
                            # Add source type sebagai watermark
                            source_text = f"Camera {camera_id} ({self.camera_sources[camera_id]})"
                            cv2.putText(image, source_text, (10, 30), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                            
                            # Save image
                            image_filename = f"cam{camera_id}_{timestamp}.jpg"
                            image_path = os.path.join(
                                self.current_session['path'],
                                "images",
                                image_filename
                            )
                            cv2.imwrite(image_path, image)
                            image_filenames[camera_id] = image_filename
                            
                        except Exception as e:
                            rospy.logerr(f"Error saving image from camera {camera_id}: {str(e)}")
                            image_filenames[camera_id] = "NA"
                    else:
                        image_filenames[camera_id] = "NA"
                
                # Write ke CSV
                data_line = (
                    f"{timestamp},"
                    f"{self.latest_gps.latitude:.6f},"
                    f"{self.latest_gps.longitude:.6f},"
                    f"{image_filenames[0]},"
                    f"{image_filenames[1]},"
                    f"{image_filenames[2]}\n"
                )
                self.current_session['data_file'].write(data_line)
                self.current_session['data_file'].flush()
                
        except Exception as e:
            rospy.logerr(f"Error in record_data: {str(e)}")
---
2024-12-24 10:35:54: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        # Existing parameters
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        
        # New parameter for video path
        self.video_path = rospy.get_param('~video_path', '')
        self.use_video = rospy.get_param('~use_video', False)
        
        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )
        
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
    def numpy_to_image_msg(self, frame):
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_video and self.video_path:
                # Check if video file exists
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                    
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Using video file: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)
                
                if not self.cap.isOpened():
                    gst_str = (
                        f"v4l2src device=/dev/video{self.camera_id} ! "
                        f"video/x-raw, width={self.width}, height={self.height} ! "
                        "videoconvert ! video/x-raw, format=BGR ! "
                        "appsink"
                    )
                    self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)
                    
            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_video else 'camera'} {self.camera_id}")
                
            if not self.use_video:
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            rospy.loginfo(f"{'Video' if self.use_video else 'Camera'} {self.camera_id} connected successfully")
            return True
            
        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_video else 'camera'} {self.camera_id}: {str(e)}")
            return False
            
    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    # For video file: loop back to beginning if ended
                    if not ret and self.use_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = self.cap.read()
                    
                    if not ret:
                        rospy.logwarn(f"Failed to capture frame from {'video' if self.use_video else 'camera'} {self.camera_id}")
                        continue
                    
                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()
            
    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                
                rospy.loginfo(f"Camera node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start camera node {self.camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error starting camera node: {str(e)}")
            
    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"Camera node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-24 10:36:05: gps:
  port: /dev/ttyTHS1
  baudrate: 9600
  update_rate: 1.0  # Hz
  frame_id: "gps"
  min_satellites: 4  
  min_signal: 25    

cameras:
  frame_rate: 30
  resolution:
    width: 640
    height: 480
  buffer_size: 1
  retry_interval: 1.0  # seconds
  # Konfigurasi untuk video lokal
  video:
    use_video: false  # Set true untuk menggunakan video lokal
    paths:
      camera0: "/path/to/your/video0.mp4"
      camera1: "/path/to/your/video1.mp4"
      camera2: "/path/to/your/video2.mp4"
  # Konfigurasi specific untuk Jetson
  gstreamer:
    enable: true
    pipeline_template: "v4l2src device=/dev/video{} ! video/x-raw, width={}, height={} ! videoconvert ! video/x-raw, format=BGR ! appsink"

recorder:
  save_path: ~/recorded_data
  record_interval: 12.0  # seconds
---
2024-12-24 10:36:23: <launch>
    <!-- Load parameter -->
    <rosparam command="load" file="$(find gps_camera_monitoring)/config/params.yaml" />
    
    <!-- GPS Node -->
    <node name="gps_node" pkg="gps_camera_monitoring" type="gps_node.py" output="screen">
        <param name="port" value="/dev/ttyTHS1"/>
        <param name="baudrate" value="9600"/>
    </node>

    <!-- Camera Node -->
    <node name="camera0_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
        <param name="camera_id" value="0"/>
        <param name="use_video" value="$(arg use_video0)" if="$(eval 'arg(\'use_video0\')' in arg)"/>
        <param name="video_path" value="$(arg video_path0)" if="$(eval 'arg(\'video_path0\')' in arg)"/>
    </node>
    <node name="camera1_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
        <param name="camera_id" value="1"/>
        <param name="use_video" value="$(arg use_video1)" if="$(eval 'arg(\'use_video1\')' in arg)"/>
        <param name="video_path" value="$(arg video_path1)" if="$(eval 'arg(\'video_path1\')' in arg)"/>
    </node>
    <node name="camera2_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
        <param name="camera_id" value="2"/>
        <param name="use_video" value="$(arg use_video2)" if="$(eval 'arg(\'use_video2\')' in arg)"/>
        <param name="video_path" value="$(arg video_path2)" if="$(eval 'arg(\'video_path2\')' in arg)"/>
    </node>

    <!-- Recorder Node -->
    <node name="recorder_node" pkg="gps_camera_monitoring" type="recorder_node.py" output="screen">
        <param name="save_path" value="$(env HOME)/recorded_data"/>
    </node>

    <!-- RQT GUI -->
    <node name="rqt_gui" pkg="rqt_gui" type="rqt_gui" output="screen"/>
</launch>
---
2024-12-24 10:40:22: Traceback (most recent call last):
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/__init__.py", line 347, in main
    p.start()
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/parent.py", line 305, in start
    self._start_infrastructure()
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/parent.py", line 254, in _start_infrastructure
    self._load_config()
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/parent.py", line 157, in _load_config
    roslaunch_strs=self.roslaunch_strs, verbose=self.verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/config.py", line 461, in load_config_default
    loader.load(f, config, argv=args, verbose=verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 761, in load
    self._load_launch(launch, ros_config, is_core=core, filename=filename, argv=argv, verbose=verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 733, in _load_launch
    self._recurse_load(ros_config, launch.childNodes, self.root_context, None, is_core, verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 671, in _recurse_load
    n = self._node_tag(tag, context, ros_config, default_machine, verbose=verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 96, in call
    return f(*args, **kwds)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 415, in _node_tag
    self._param_tag(t, param_ns, ros_config, force_local=True, verbose=verbose)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 95, in call
    if ifunless_test(args[0], args[1], args[2]):
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 74, in ifunless_test
    if_val, unless_val = obj.opt_attrs(tag, context, ['if', 'unless'])
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 208, in opt_attrs
    return [self.resolve_args(tag_value(tag,a), context) for a in attrs]
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/xmlloader.py", line 189, in resolve_args
    return substitution_args.resolve_args(args, context=context.resolve_dict, resolve_anon=self.resolve_anon)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/substitution_args.py", line 378, in resolve_args
    return _eval(arg_str[7:-1], context)
  File "/opt/ros/melodic/lib/python2.7/dist-packages/roslaunch/substitution_args.py", line 347, in _eval
    return str(eval(s, {}, _DictWrapper(context['arg'], functions)))
  File "<string>", line 1, in <module>
TypeError: argument of type 'function' is not iterable
---
2024-12-24 10:46:27: import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)

        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        self.input_type = rospy.get_param('~input_type', 'webcam')  # webcam atau video
        self.video_path = rospy.get_param('~video_path', '')

        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )

        self.cap = None
        self.running = False
        self.lock = threading.Lock()

    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message tanpa cv_bridge"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.input_type == 'webcam':
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)

                if not self.cap.isOpened():
                    raise Exception(f"Could not open camera {self.camera_id}")

            elif self.input_type == 'video':
                if not self.video_path:
                    raise Exception("Video path is not specified")
                self.cap = cv2.VideoCapture(self.video_path)

                if not self.cap.isOpened():
                    raise Exception(f"Could not open video file: {self.video_path}")

            rospy.loginfo(f"Camera {self.camera_id} connected successfully (Input: {self.input_type})")
            return True

        except Exception as e:
            rospy.logerr(f"Failed to connect to input source {self.input_type}: {str(e)}")
            return False

    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue

                    ret, frame = self.cap.read()

                    if not ret:
                        rospy.logwarn(f"Failed to capture frame from {self.input_type}")
                        if self.input_type == 'video':
                            rospy.loginfo("Restarting video from beginning")
                            self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        continue

                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)

            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)

            rate = rospy.Rate(self.frame_rate)
            rate.sleep()

    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()

                rospy.loginfo(f"Camera node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start camera node {self.camera_id}")

        except Exception as e:
            rospy.logerr(f"Error starting camera node: {str(e)}")

    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"Camera node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-24 10:53:33: import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)

        # Mengambil parameter dari namespace global
        self.camera_id = rospy.get_param('/camera/camera_id', 0)
        self.frame_rate = rospy.get_param('/camera/frame_rate', 30)
        self.width = rospy.get_param('/camera/width', 640)
        self.height = rospy.get_param('/camera/height', 480)
        self.input_type = rospy.get_param('/camera/input_type', 'webcam')  # webcam atau video
        self.video_path = rospy.get_param('/camera/video_path', '')

        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )

        self.cap = None
        self.running = False
        self.lock = threading.Lock()

    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message tanpa cv_bridge"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.input_type == 'webcam':
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)

                if not self.cap.isOpened():
                    raise Exception(f"Could not open camera {self.camera_id}")

            elif self.input_type == 'video':
                if not self.video_path:
                    raise Exception("Video path is not specified")
                self.cap = cv2.VideoCapture(self.video_path)

                if not self.cap.isOpened():
                    raise Exception(f"Could not open video file: {self.video_path}")

            rospy.loginfo(f"Camera {self.camera_id} connected successfully (Input: {self.input_type})")
            return True

        except Exception as e:
            rospy.logerr(f"Failed to connect to input source {self.input_type}: {str(e)}")
            return False

    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue

                    ret, frame = self.cap.read()

                    if not ret:
                        rospy.logwarn(f"Failed to capture frame from {self.input_type}")
                        if self.input_type == 'video':
                            rospy.loginfo("Restarting video from beginning")
                            self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        continue

                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)

            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)

            rate = rospy.Rate(self.frame_rate)
            rate.sleep()

    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()

                rospy.loginfo(f"Camera node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start camera node {self.camera_id}")

        except Exception as e:
            rospy.logerr(f"Error starting camera node: {str(e)}")

    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"Camera node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-24 11:33:15: checkpoint = torch.load(args.resume, map_location='cpu')
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch/references/deploy/video.py", line 170, in <module>
    main(args)
  File "/home/ris/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch/references/deploy/video.py", line 48, in main
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/serialization.py", line 1319, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/serialization.py", line 659, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/serialization.py", line 640, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'output/checkpoint0119.pth'
---
2024-12-24 11:33:53: import sys
# import os
sys.path.append(r'/home/ris/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch')
import cv2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as T
from PIL import Image, ImageDraw
from src.core import YAMLConfig
import time  # Tambahkan modul time untuk menghitung FPS

# Pemetaan kelas dari model
class_mapping = {
    0: "AC", 1: "BC", 2: "BL", 3: "BS", 4: "CR", 5: "DP", 6: "EC",
    7: "JC", 8: "LS", 9: "LT", 10: "PA", 11: "PC", 12: "PH", 13: "RT",
    14: "SC", 15: "SL", 16: "SV", 17: "WR"
}

# Fungsi untuk menggambar hasil prediksi pada frame
def draw(im, labels, boxes, scores, thrh=0.6):
    draw = ImageDraw.Draw(im)

    scr = scores
    lab = labels[scr > thrh]
    box = boxes[scr > thrh]
    scrs = scr[scr > thrh]

    for j, b in enumerate(box):
        # Draw bounding box
        draw.rectangle(list(b), outline='red', width=2)
        # Get class name from mapping
        class_id = lab[j].item()
        class_name = class_mapping.get(class_id, "Unknown")
        score = round(scrs[j].item(), 2)
        # Display class name and score
        label = f"{class_name} {score}"
        text_position = (b[0], b[1] - 10)  # Posisi di atas bounding box
        draw.text(text_position, label, fill='blue')

def main(args):
    """main"""
    # Load konfigurasi model
    cfg = YAMLConfig(args.config, resume=args.resume)

    # Load checkpoint model
    if args.resume:
        checkpoint = torch.load(args.resume, map_location='cpu')
        if 'ema' in checkpoint:
            state = checkpoint['ema']['module']
        else:
            state = checkpoint['model']
    else:
        raise AttributeError('Only support resume to load model.state_dict by now.')

    # Load state_dict ke model
    cfg.model.load_state_dict(state)

    # Definisikan model untuk inferensi
    class Model(nn.Module):
        def __init__(self):
            super().__init__()
            self.model = cfg.model.deploy()
            self.postprocessor = cfg.postprocessor.deploy()

        def forward(self, images, orig_target_sizes):
            outputs = self.model(images)
            outputs = self.postprocessor(outputs, orig_target_sizes)
            return outputs

    model = Model().to(args.device)
    model.eval()  # Set model ke mode evaluasi

    # Buka video menggunakan OpenCV
    cap = cv2.VideoCapture(args.im_file)
    if not cap.isOpened():
        raise FileNotFoundError(f"Error: Cannot open video file {args.im_file}")

    # Konfigurasi output video
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))

    # Transformasi untuk input ke model
    transforms = T.Compose([
        T.Resize((640, 640)),
        T.ToTensor(),
    ])

    # Variabel untuk menghitung FPS
    frame_count = 0
    start_time = time.time()

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Hitung waktu mulai pemrosesan frame
        frame_start = time.time()

        # Convert frame ke PIL Image
        im_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        w, h = im_pil.size
        orig_size = torch.tensor([w, h])[None].to(args.device)

        # Transform frame
        im_data = transforms(im_pil)[None].to(args.device)

        # Inferensi menggunakan model
        with torch.no_grad():
            output = model(im_data, orig_size)
            labels, boxes, scores = output

        # Draw predictions pada frame
        draw(im_pil, labels[0], boxes[0], scores[0])

        # Convert back to numpy array
        annotated_frame = cv2.cvtColor(np.array(im_pil), cv2.COLOR_RGB2BGR)

        # Hitung waktu akhir pemrosesan frame
        frame_end = time.time()
        frame_time = frame_end - frame_start
        fps_current = 1 / frame_time

        # Tambahkan FPS ke frame
        cv2.putText(
            annotated_frame,
            f"FPS: {fps_current:.2f}",
            (10, 30),  # Posisi teks di frame
            cv2.FONT_HERSHEY_SIMPLEX,  # Font
            1,  # Ukuran font
            (0, 255, 0),  # Warna teks (hijau)
            2,  # Ketebalan garis
            cv2.LINE_AA
        )

        # Tulis frame ke output video
        out.write(annotated_frame)

        # Tampilkan hasil (opsional)
        cv2.imshow('Predictions', annotated_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        # Tambahkan frame count
        frame_count += 1

    # Hitung FPS rata-rata
    end_time = time.time()
    total_time = end_time - start_time
    avg_fps = frame_count / total_time
    print(f"Average FPS: {avg_fps:.2f}")

    # Release resources
    cap.release()
    out.release()
    cv2.destroyAllWindows()

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, required=True, help='Path to config file')
    parser.add_argument('-r', '--resume', type=str, required=True, help='Path to model checkpoint')
    parser.add_argument('-f', '--im-file', type=str, required=True, help='Path to input video')
    parser.add_argument('-d', '--device', type=str, default='cpu', help='Device to run inference on')
    args = parser.parse_args()
    main(args)
---
2024-12-24 11:35:44: Load PResNet18 state_dict
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch/references/deploy/video.py", line 170, in <module>
    main(args)
  File "/home/ris/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch/references/deploy/video.py", line 71, in main
    model = Model().to(args.device)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
  File "/home/ris/gps_camera_ws/ris_2_transformer/lib/python3.9/site-packages/torch/cuda/__init__.py", line 310, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled
---
2024-12-24 11:37:37: (ris_2_transformer) ris@ris-desktop:~/gps_camera_ws/ML/train/TRANS_1/RT-DETR/rtdetrv2_pytorch$ python references/deploy/video.py -c configs/rtdetrv2/rtdetrv2_r18vd_120e_coco.yml -r output1/checkpoint0119.pth -f 1211.mp4 -d cuda
---
2024-12-24 11:57:08: (ris_2_faster) ris@ris-desktop:~/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch$ python run.py
/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:
  warn(f"Failed to load image Python extension: {e}")
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/run.py", line 38, in <module>
    model.load_state_dict(torch.load(r"/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"))
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 152, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 136, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
---
2024-12-24 11:57:25: import cv2
import torch
from torchvision import transforms
from PIL import Image
from model import FasterRCNN
import time

# Muat model_config dari parameter yang telah ditentukan sebelumnya
model_config = {
    "im_channels": 3,
    "aspect_ratios": [0.5, 1, 2],
    "scales": [128, 256, 512],
    "min_im_size": 600,
    "max_im_size": 1000,
    "backbone_out_channels": 512,
    "fc_inner_dim": 1024,
    "rpn_bg_threshold": 0.3,
    "rpn_fg_threshold": 0.7,
    "rpn_nms_threshold": 0.7,
    "rpn_train_prenms_topk": 12000,
    "rpn_test_prenms_topk": 6000,
    "rpn_train_topk": 2000,
    "rpn_test_topk": 300,
    "rpn_batch_size": 256,
    "rpn_pos_fraction": 0.5,
    "roi_iou_threshold": 0.5,
    "roi_low_bg_iou": 0.0,
    "roi_pool_size": 7,
    "roi_nms_threshold": 0.3,
    "roi_topk_detections": 100,
    "roi_score_threshold": 0.05,
    "roi_batch_size": 128,
    "roi_pos_fraction": 0.25,
}

# Muat model
model = FasterRCNN(model_config, num_classes=19)  # Sesuaikan jumlah kelas
model.load_state_dict(torch.load(r"/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"))
model.eval()

# Pastikan model berada di perangkat yang sesuai (GPU atau CPU)
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device( "cpu")
model = model.to(device)

# Daftar nama kelas
class_names = ["background", 'AC', 'BC', 'EC', 'JC', 'LT', 'SC', 'BS', 'CR', 'DP', 'LS', 'PH', 'RT', 'SV', 'SL', 'PA', 'BL', 'WR', 'PC']

# Transformasi untuk preprocessing gambar (resize, normalize)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Fungsi untuk memproses frame dan melakukan prediksi
def process_frame(frame):
    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    input_image = transform(image).unsqueeze(0).to(device)  # Tambahkan batch dimension

    with torch.no_grad():
        _, frcnn_output = model(input_image)

    boxes = frcnn_output['boxes'].cpu().numpy()
    scores = frcnn_output['scores'].cpu().numpy()
    labels = frcnn_output['labels'].cpu().numpy()

    return boxes, scores, labels

# Fungsi untuk menggambar deteksi pada frame
def draw_detections(frame, boxes, scores, labels, score_threshold=0.5):
    for box, score, label in zip(boxes, scores, labels):
        if score < score_threshold:
            continue

        x1, y1, x2, y2 = box.astype(int)
        class_name = class_names[label] if label < len(class_names) else f"class{label}"
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f"{class_name}: {score:.2f}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Buka video
video_path = r"/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/1223.mp4"
cap = cv2.VideoCapture(video_path)

# Tentukan format output video
output_path = r"/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/outputh32.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Variabel untuk menghitung FPS rata-rata
total_time = 0
frame_count = 0

# Proses setiap frame
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Waktu mulai untuk frame ini
    frame_start_time = time.time()

    # Lakukan prediksi
    boxes, scores, labels = process_frame(frame)

    # Gambar hasil deteksi pada frame
    draw_detections(frame, boxes, scores, labels)

    # Hitung waktu proses untuk frame ini
    frame_end_time = time.time()
    frame_time = frame_end_time - frame_start_time
    total_time += frame_time

    # Hitung FPS untuk frame ini
    fps = 1 / frame_time if frame_time > 0 else 0
    frame_count += 1
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

    # Tampilkan hasil di layar (opsional)
    cv2.imshow('Video', frame)

    # Simpan frame ke file output
    out.write(frame)

    # Tekan 'q' untuk keluar
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Hitung rata-rata FPS setelah semua frame diproses
average_fps = frame_count / total_time if total_time > 0 else 0

# Tampilkan hasil rata-rata FPS
print(f"Total Frames Processed: {frame_count}")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average FPS: {average_fps:.2f}")

# Bersihkan resource
cap.release()
out.release()
cv2.destroyAllWindows()
---
2024-12-24 12:10:18: /home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:
  warn(f"Failed to load image Python extension: {e}")
Traceback (most recent call last):
  File "/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/run.py", line 38, in <module>
    model.load_state_dict(torch.load(r"/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"))
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 712, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1046, in _load
    result = unpickler.load()
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1016, in persistent_load
    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 1001, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 176, in default_restore_location
    result = fn(storage, location)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 152, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/home/ris/gps_camera_ws/ris_2_faster/lib/python3.9/site-packages/torch/serialization.py", line 136, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
---
2024-12-24 20:01:02: alias cdp='cd -'  # Go to the previous directory
alias cdup='cd ..'  # Go up one directory
alias cdupp='cd ../..'  # Go up two directories

# Save a "shortcut" for frequently visited directories
alias proj='cd ~/Projects/myproject'
alias docs='cd ~/Documents'

# Bookmark and quickly switch directories
alias mark='pwd > ~/.dir_marker'  # Mark current directory
alias go='cd $(cat ~/.dir_marker)'  # Go to the marked directory
---
2024-12-24 20:02:25: alias cpwd='pwd | xclip -selection clipboard'  

# Copy a file's full path to clipboard
alias cpf='realpath $1 | pbcopy'  # macOS
alias cpf='realpath $1 | xclip -selection clipboard'  # Linux
alias cpf='realpath $1 | clip'  # Windows with WSL
---
2024-12-24 20:04:15: # Directory Bookmarks File
BOOKMARKS_FILE=~/.dir_bookmarks

# Add a directory to bookmarks
mark() {
  local dir="${1:-$(pwd)}"
  local name="${2:-$(basename "$dir")}"
  echo "$name|$dir" >> "$BOOKMARKS_FILE"
  sort -u -o "$BOOKMARKS_FILE" "$BOOKMARKS_FILE"
  echo "Marked '$dir' as '$name'."
}

# Go to a bookmarked directory
go() {
  if [[ -s "$BOOKMARKS_FILE" ]]; then
    local choice
    choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Bookmarks: " --height=15 --reverse)
    [[ -n "$choice" ]] && cd "$(awk -F '|' -v name="$choice" '$1 == name {print $2}' "$BOOKMARKS_FILE")" || echo "No bookmark selected."
  else
    echo "No bookmarks found."
  fi
}

# List all bookmarks
marks() {
  column -t -s '|' "$BOOKMARKS_FILE" | less
}

# Remove a bookmark
unmark() {
  local choice
  choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Remove bookmark: " --height=15 --reverse)
  if [[ -n "$choice" ]]; then
    grep -v "^$choice|" "$BOOKMARKS_FILE" > "${BOOKMARKS_FILE}.tmp" && mv "${BOOKMARKS_FILE}.tmp" "$BOOKMARKS_FILE"
    echo "Removed bookmark '$choice'."
  else
    echo "No bookmark removed."
  fi
}
---
2024-12-24 20:05:04: # File for bookmarks
BOOKMARKS_FILE=~/.dir_bookmarks

# Add a directory to bookmarks
mark() {
  local dir="${1:-$(pwd)}"
  local name="${2:-$(basename "$dir")}"
  echo "$name|$dir" >> "$BOOKMARKS_FILE"
  sort -u -o "$BOOKMARKS_FILE" "$BOOKMARKS_FILE"
  echo "Marked '$dir' as '$name'."
}

# Go to a bookmarked directory
go() {
  if [[ -s "$BOOKMARKS_FILE" ]]; then
    local choice
    choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Bookmarks: " --height=15 --reverse)
    [[ -n "$choice" ]] && cd "$(awk -F '|' -v name="$choice" '$1 == name {print $2}' "$BOOKMARKS_FILE")" || echo "No bookmark selected."
  else
    echo "No bookmarks found."
  fi
}

# Remove a bookmark
unmark() {
  local choice
  choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Remove bookmark: " --height=15 --reverse)
  if [[ -n "$choice" ]]; then
    grep -v "^$choice|" "$BOOKMARKS_FILE" > "${BOOKMARKS_FILE}.tmp" && mv "${BOOKMARKS_FILE}.tmp" "$BOOKMARKS_FILE"
    echo "Removed bookmark '$choice'."
  else
    echo "No bookmark removed."
  fi
}

# Key Bindings
bind -x '"\C-m": mark'     # Ctrl+m to add the current directory as a bookmark
bind -x '"\C-g": go'       # Ctrl+g to jump to a bookmark
bind -x '"\C-u": unmark'   # Ctrl+u to remove a bookmark
---
2024-12-24 20:25:45: # File to store bookmarks
BOOKMARKS_FILE=~/.dir_bookmarks

# Add a directory to bookmarks
mark() {
  local dir="${1:-$(pwd)}"
  local name="${2:-$(basename "$dir")}"
  echo "$name|$dir" >> "$BOOKMARKS_FILE"
  sort -u -o "$BOOKMARKS_FILE" "$BOOKMARKS_FILE"
  echo "Marked '$dir' as '$name'."
}

# Navigate to a bookmarked directory
go() {
  if [[ -s "$BOOKMARKS_FILE" ]]; then
    local choice
    choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Bookmarks: " --height=15 --reverse)
    if [[ -n "$choice" ]]; then
      local dir
      dir=$(awk -F '|' -v name="$choice" '$1 == name {print $2}' "$BOOKMARKS_FILE")
      cd "$dir" || echo "Failed to navigate to '$dir'."
    else
      echo "No bookmark selected."
    fi
  else
    echo "No bookmarks found."
  fi
}

# List all bookmarks
marks() {
  column -t -s '|' "$BOOKMARKS_FILE" | less
}

# Remove a bookmark
unmark() {
  local choice
  choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Remove bookmark: " --height=15 --reverse)
  if [[ -n "$choice" ]]; then
    grep -v "^$choice|" "$BOOKMARKS_FILE" > "${BOOKMARKS_FILE}.tmp" && mv "${BOOKMARKS_FILE}.tmp" "$BOOKMARKS_FILE"
    echo "Removed bookmark '$choice'."
  else
    echo "No bookmark removed."
  fi
}
---
2024-12-24 20:29:58: sudo cp /bin/bash /bin/bash.bak
---
2024-12-24 20:30:27: -bash: /home/ris/.bashrc: line 127: syntax error near unexpected token `('
-bash: /home/ris/.bashrc: line 127: `mark () {'
---
2024-12-24 20:31:14: # File for storing bookmarks
BOOKMARKS_FILE=~/.dir_bookmarks

# Function to add a bookmark
mark() {
    local dir="${1:-$(pwd)}"
    local name="${2:-$(basename "$dir")}"
    echo "$name|$dir" >> "$BOOKMARKS_FILE"
    sort -u -o "$BOOKMARKS_FILE" "$BOOKMARKS_FILE"
    echo "Marked '$dir' as '$name'."
}

# Function to go to a bookmarked directory
go() {
    if [[ -s "$BOOKMARKS_FILE" ]]; then
        local choice
        choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Bookmarks: " --height=15 --reverse)
        [[ -n "$choice" ]] && cd "$(awk -F '|' -v name="$choice" '$1 == name {print $2}' "$BOOKMARKS_FILE")" || echo "No bookmark selected."
    else
        echo "No bookmarks found."
    fi
}

# Function to remove a bookmark
unmark() {
    local choice
    choice=$(awk -F '|' '{print $1}' "$BOOKMARKS_FILE" | fzf --prompt="Remove bookmark: " --height=15 --reverse)
    if [[ -n "$choice" ]]; then
        grep -v "^$choice|" "$BOOKMARKS_FILE" > "${BOOKMARKS_FILE}.tmp" && mv "${BOOKMARKS_FILE}.tmp" "$BOOKMARKS_FILE"
        echo "Removed bookmark '$choice'."
    else
        echo "No bookmark removed."
    fi
}

# Bind keys to functions
bind -x '"\C-m": mark'
bind -x '"\C-g": go'
bind -x '"\C-u": unmark'
---
2024-12-24 20:38:53: map Y :echo -n $(pwd)/$f | xclip -selection clipboard
---
2024-12-24 20:39:02: # Yank full path of current file/directory
map Y :echo -n $(pwd)/$f | xclip -selection clipboard

# Bind Ctrl+G to fzf-based directory navigation
map <c-g> :fzf-cd

# Define fzf-cd command
cmd fzf-cd %{{
    cd "$(find . -type d | fzf)" || exit
}}
---
2024-12-25 18:58:50: # vim: filetype=sh
# Luke's lf settings

# Basic vars
set shell bash
set shellopts '-eu'
set ifs "\n"
set scrolloff 10
set icons
set period 1
# set hiddenfiles ".*:*.aux:*.log:*.bbl:*.bcf:*.blg:*.run.xml"

# set ratios .5:1:2
set relativenumber!
# for border
# set drawbox!
# set info size:ctime

# ===============================
set cleaner '~/.config/lf/cleaner'
set previewer '~/.config/lf/scope'
# OR ===============================
# Vars that depend on environmental variables
# $lf -remote "send $id set previewer ${XDG_CONFIG_HOME:-$HOME/.config}/lf/scope"
# ===============================

# cmds/functions
# cmd open ${{
#   case $(file --mime-type $f -b) in
# 	image/vnd.djvu|application/pdf|application/octet-stream) setsid -f zathura $fx >/dev/null 2>&1 ;;
#   text/*) $EDITOR $fx;;
# 	image/x-xcf) setsid -f gimp $f >/dev/null 2>&1 ;;
# 	image/svg+xml) display -- $f ;;
# 	image/*) sxiv $f 2>/dev/null ;;
# 	audio/*) mpv --audio-display=no $f ;;
# 	video/*) setsid -f mpv $f -quiet >/dev/null 2>&1 ;;
# 	application/pdf|application/vnd*|application/epub*) setsid -f zathura $fx >/dev/null 2>&1 ;;
# 	application/pgp-encrypted) $EDITOR $fx ;;
#         *) for f in $fx; do setsid -f $OPENER $f >/dev/null 2>&1; done;;
#     esac
# }}

cmd mkdir $mkdir -p "$(echo $* | tr ' ' '\ ')"
cmd touch $touch $*

# cmd zlLinkSymbolLink ${{
# 	# clear; tput cup $(($(tput lines)/3)); tput bold
# 	# set -f
# 	printf "%s\n\t" "$(func-clipboard p)"
# 	# printf "extracts?[y/N]"
# 	# read ans
# 	# [ $ans = "y" ] && ln -s "$(func-clipboard p)" .
#   ln -s "$(func-clipboard p)" .
# }}

cmd extracts ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	printf "extracts?[y/N]"
	read ans
	[ $ans = "y" ] && extract "$(realpath -s "$fx")"
}}

cmd delete ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	printf "delete?[y/N]"
	read ans
	[ $ans = "y" ] && rm -rf -- $fx
}}

# cmd trash ${{
#     mkdir -p ~/.trash
#     IFS="'printf '\n\t''"; mv "$fx" ~/.trash
# }}

# cmd moveto ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
# 	clear; echo "Move to where?"
#   source $HOME/.config/zsh/custom/plugins/formarks.plugin.zsh
#   buf_path=$(jmark_api)
#   # printf "$buf_path" | func-clipboard c 
# 
# 	# dest="$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')" &&
# 	# for x in $fx; do
# 	# 	eval mv -iv \"$x\" \"$dest\"
# 	# done &&
# 	# notify-send "🚚 File(s) moved." "File(s) moved to $dest."
# }}

# cmd moveto ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
# 	clear; echo "Move to where?"
# 	dest="$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')" &&
# 	for x in $fx; do
# 		eval mv -iv \"$x\" \"$dest\"
# 	done &&
# 	notify-send "🚚 File(s) moved." "File(s) moved to $dest."
# }}

# cmd copyto ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
# 	clear; echo "Copy to where?"
# 	dest="$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')" &&
#   [ -z $dest ] && exit
# 	for x in $fx; do
# 		eval cp -ivr \"$x\" \"$dest\"
# 	done &&
# 	notify-send "📋 File(s) copied." "File(s) copies to $dest."
# }}

# cmd imageMergeCmdVer ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
#   {
#     imageMergeRes=$(echo "$fx" | tr '\n' '@')
#     image-join -v "$imageMergeRes"
#     notify-send "📋File(s) joined $imageMergeRes ."
#   } || {
#     clear; echo "fail"
#   }
# }}
# 
# cmd imageMergeCmd ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
# 	# clear; $(echo "$fx" | tr '\n' ' ')
#   # searchFile="$(realpath -s "$fx")"
#   # basesearch="$(basename "$searchFile")"
#   # secondFile="${searchFile/$basesearch/}"
#   # dest="$(find "$secondFile" -type f | fzf )"
#   # image-join "$fx" "$dest"
#   # [ -z $dest ] && exit
# 	# for x in $fx; do
# 	# 	eval cp -ivr \"$x\" \"$dest\"
# 	# done &&
# 	# notify-send "📋 File(s) copied." "File(s) copies to $dest."
#   {
#     imageMergeRes=$(echo "$fx" | tr '\n' '@')
#     image-join "$imageMergeRes"
#     notify-send "📋File(s) joined $imageMergeRes ."
#   } || {
#     clear; echo "fail"
#   }
# }}

# cmd gotoCmd ${{
# 	clear; tput cup $(($(tput lines)/3)); tput bold
# 	set -f
# 	clear; echo "where to where?"
# 	dest="$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')" &&
#   dest="\"$dest\""
#   [ -z $dest ] && exit
#   echo $dest >> ~/lfrc.txt
#   $lf -remote "send '$id' cd '$dest' "
# }}

cmd openType ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	echo "filetype $(xdg-mime query filetype "$f")"
	echo "default app $(xdg-mime query default $(xdg-mime query filetype "$f"))"
	read ans
	# [ $ans = "y" ] && rm -rf -- $fx
}}

cmd launchCmdDevour ${{
  launch "@" "$f"
}}
cmd launchCmd ${{
  launch "$f"
  # if [[ $(xdg-mime query default $(xdg-mime query filetype "$f")) == "" ]];then
  #     launch "$f"
  # else
  #   setsid -f xdg-open "$f" >/dev/null 2>&1
  # fi
}}

cmd bulkrename ${{
  /bin/sh -c "vimv $(echo -e "$fx" | xargs -i echo "\\'{}\\'" | xargs echo)"
}}

# cmd fasd_dir ${{
#    res="$(fasd -dl | grep -iv cache | fzf 2>/dev/tty | sed 's/\\/\\\\/g;s/"/\\"/g')"
#    if [ -d "$res" ]; then
#       cmd="cd"
#    else
#       cmd="select"
#    fi
#    [[ -z "$res" ]] && return
#  lf -remote "send $id $cmd \"$res\""
#  }}

# cmd FzfFormarksCp ${{
#     source1=$( sed 's#: # -> #' "$HOME/.local/share/ezel/lib/pathmarks"| nl| sed -E 's/^\s*//g' )
#     source2=$(cat ~/.z | grep -Po "^.+(?=\|[0-9]+\|)")
# 
#     output=$( { printf "$source1\n" && printf "$source2"; }  |
#       fzf --query="$*" -1|
#       sed 's#.*->\s*##')
# 
#     # echo $output $fx
#     [[ -z "$output" ]] && exit
# 
#     for x in $fx; do
#       eval cp -ivr \"$x\" \"$output\"
#     done &&
#     notify-send "🚚 File(s) moved." "File(s) moved to $output."
#  }}

cmd LfTmuxNewTab ${{
    source "$HOME/.config/lf/lfcd.sh"
    tmux neww -a "lfcd.bak"
}}

# cmd FzfFormarksMv ${{
#     source1=$( sed 's#: # -> #' "$HOME/.local/share/.pathmarks"| nl| sed -E 's/^\s*//g' )
#     source2=$(cat ~/.z | grep -Po "^.+(?=\|[0-9]+\|)")
# 
#     output=$( { printf "$source1\n" && printf "$source2"; }  |
#       fzf --query="$*" -1|
#       sed 's#.*->\s*##')
# 
#     # echo $output $fx
#     [[ -z "$output" ]] && return
# 
#     for x in $fx; do
#       eval mv -iv \"$x\" \"$output\"
#     done &&
#     notify-send "🚚 File(s) moved." "File(s) moved to $output."
#  }}

cmd FzfFormarks ${{
    source1=$( sed 's#: # -> #' "$HOME/.local/share/ezel/lib/pathmarks"| nl| sed -E 's/^\s*//g' )
    source2=$(cat ~/.z  | sed -r "s/\|.*//g" )

    output=$( { printf "$source1\n" && printf "$source2"; } |
      fzf --query="$*" -1|
      sed 's#.*->\s*##')

    echo $output
    if [ -d "$output" ]; then
      cmd="cd"
    else
      cmd="select"
    fi
   [[ -z "$output" ]] && return
  lf -remote "send $id $cmd \"$output\""
 }}

# cmd FzfFormarksMark ${{
#    clear; tput cup $(($(tput lines)/3)); tput bold
#    set -f
#    printf "name :"
#    read ans
#    [[ -z "$ans" ]] && return
#    mark_to_add=$(echo "$ans: $(pwd)")
#     echo "${mark_to_add}" >> "$HOME/.local/share/.pathmarks"
#     echo "** The following mark has been added **"
#     echo "${mark_to_add}"
#  }}

cmd FzfAutoJump ${{
   ans="$(cat ~/.local/share/autojump/autojump.txt | awk '{$1 = ""; print $0}' | sed 's/^ *//g' | fzf 2>/dev/tty )"
   echo $ans
   source ~/.local/bin/autojump.zsh
   output="$(autojump "$ans")"
   if [ -d "$output" ]; then
      cmd="cd"
   else
      cmd="select"
   fi
   [[ -z "$output" ]] && return
 lf -remote "send $id $cmd \"$output\""
 }}

cmd autojump ${{
   clear; tput cup $(($(tput lines)/3)); tput bold
   set -f
   printf "%s\n\t" "$fx"
   printf "input :"
   read ans
   output="$(autojump ${ans})"
   if [ -d "$output" ]; then
      cmd="cd"
   else
      cmd="select"
   fi
   [[ -z "$output" ]] && return
 lf -remote "send $id $cmd \"$output\""
 }}

# cmd setbg "$1"
# cmd bulkrename $vidir
#


# # Bindings
map <c-f> $lf -remote "send $id select '$(fd --follow --hidden --color=always | fzf --ansi)'"
# map <c-f> $lf -remote "send $id select '$(fd --follow --hidden | fzf )'"
# map J $lf -remote "send $id cd $(sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs | fzf)"

map y
map yy copy
map yp $echo -n "$f" | func-clipboard c
map yd $echo -n $(dirname "$f") | func-clipboard c
map yn $echo -n $(basename "$f") | func-clipboard c

map gh
map gg top

map d
map dd cut
map dD delete
map dt trash
map dc %du -sh $f

map xe $extract "$f"

# map C copyto
# map M moveto
# map zM moveto

# map a push :touch<space>
# map A push :mkdir<space>
# map <c-n> push $func-alias-newFile<space>
map <c-n> push $touch <space>
# map <c-s-n> push $func-alias-newDirectory<space> # TODO: if ctrl shift map already a feature

# map <a-F> push $_zsh_func_ezel


map <c-r> reload

# map zz :FzfAutoJump
map <c-g> :FzfFormarks
# map zz :FzfFormarks
# map zf :FzfFormarksMark
# map zM :FzfFormarksMv
# map zC :FzfFormarksCp
map zt :LfTmuxNewTab
# map zl :zlLinkSymbolLink

map u
map uy clear
map uv unselect

map <enter> shell
map xx $$f

map o
map otw $tmux neww -c $PWD
map ote $tmux neww -c $PWD "lfcd"
map ots $tmux-sessionizer $PWD
map od $launch $PWD
map oo launchCmd $f
map os launchCmdDevour $f
map ott openType $f

map <c-o> $lf -remote "send $id cd \"$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')\""

map i
map ij imageMergeCmd

map A rename # at the very end
# map a push A<a-b> # after extention
map a push $create<space>
# map I push A<c-a> # after extention
map <f-2> push A # after extention
map I push A<a-b><a-b><a-f> # before extension

# map B bulkrename
# map X !$f
# map o &mimeopen $f
# map O $mimeopen --ask $f
# map O $mimeopen --ask $f
# map <c-h> set hidden!
# map c push A<c-u> # new rename
# map I push A<c-a> # at the very beginning
# map i push A<a-b><a-b><a-f> # before extention
# map b $setbg $f

# map <c-d> down
# map <c-u> up
map <c-e> push :!nvim<space>

map W $setsid -f $TERMINAL >/dev/null 2>&1

# Source Bookmarks
source "~/.config/shell/lfshortcuts"
#
#
# cmd bulkrename ${{
# 	index=$(mktemp /tmp/lf-bulk-rename-index.XXXXXXXXXX)
# 	if [ -n "${fs}" ]; then
# 		echo "$fs" > $index
# 	else
# 		echo "$(ls "$(dirname $f)" | tr ' ' "\n")" > $index
# 	fi
# 	index_edit=$(mktemp /tmp/lf-bulk-rename.XXXXXXXXXX)
# 	cat $index > $index_edit
# 	$EDITOR $index_edit
# 	if [ $(cat $index | wc -l) -eq $(cat $index_edit | wc -l) ]; then
# 		max=$(($(cat $index | wc -l)+1))
# 		counter=1
# 		while [ $counter -le $max ]; do
# 			a="$(cat $index | sed "${counter}q;d")"
# 			b="$(cat $index_edit | sed "${counter}q;d")"
# 			counter=$(($counter+1))
#
# 			[ "$a" = "$b" ] && continue
# 			[ -e "$b" ] && echo "File exists: $b" && continue
# 			mv "$a" "$b"
# 		done
# 	else
# 		echo "Number of lines must stay the same"
# 	fi
# 	rm $index $index_edit
# }}
# set shellopts '-eu'
# set ifs "\n"
---
2024-12-25 19:14:30: # vim: filetype=sh
# Luke's lf settings

# Basic vars
set shell bash
set shellopts '-eu'
set ifs "\n"
set scrolloff 10
#set icons
set period 1
# set hiddenfiles ".*:*.aux:*.log:*.bbl:*.bcf:*.blg:*.run.xml"

# set ratios .5:1:2
set relativenumber!
# for border
# set drawbox!
# set info size:ctime

# ===============================
# set cleaner '~/.config/lf/cleaner'
# set previewer '~/.config/lf/scope'
# OR ===============================
# Vars that depend on environmental variables
# $lf -remote "send $id set previewer ${XDG_CONFIG_HOME:-$HOME/.config}/lf/scope"
# ===============================

# cmds/functions
# cmd open ${{
#   case $(file --mime-type $f -b) in
# 	image/vnd.djvu|application/pdf|application/octet-stream) setsid -f zathura $fx >/dev/null 2>&1 ;;
#   text/*) $EDITOR $fx;;
# 	image/x-xcf) setsid -f gimp $f >/dev/null 2>&1 ;;
# 	image/svg+xml) display -- $f ;;
# 	image/*) sxiv $f 2>/dev/null ;;
# 	audio/*) mpv --audio-display=no $f ;;
# 	video/*) setsid -f mpv $f -quiet >/dev/null 2>&1 ;;
# 	application/pdf|application/vnd*|application/epub*) setsid -f zathura $fx >/dev/null 2>&1 ;;
# 	application/pgp-encrypted) $EDITOR $fx ;;
#         *) for f in $fx; do setsid -f $OPENER $f >/dev/null 2>&1; done;;
#     esac
# }}

cmd mkdir $mkdir -p "$(echo $* | tr ' ' '\ ')"
cmd touch $touch $*

cmd extracts ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	printf "extracts?[y/N]"
	read ans
	[ $ans = "y" ] && extract "$(realpath -s "$fx")"
}}

cmd delete ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	printf "delete?[y/N]"
	read ans
	[ $ans = "y" ] && rm -rf -- $fx
}}

cmd trash ${{
    mkdir -p ~/.trash
    IFS="'printf '\n\t''"; mv "$fx" ~/.trash
}}

cmd openType ${{
	clear; tput cup $(($(tput lines)/3)); tput bold
	set -f
	printf "%s\n\t" "$fx"
	echo "filetype $(xdg-mime query filetype "$f")"
	echo "default app $(xdg-mime query default $(xdg-mime query filetype "$f"))"
	read ans
	# [ $ans = "y" ] && rm -rf -- $fx
}}

cmd launchCmdDevour ${{
  launch "@" "$f"
}}
cmd launchCmd ${{
  launch "$f"
  # if [[ $(xdg-mime query default $(xdg-mime query filetype "$f")) == "" ]];then
  #     launch "$f"
  # else
  #   setsid -f xdg-open "$f" >/dev/null 2>&1
  # fi
}}

cmd bulkrename ${{
  /bin/sh -c "vimv $(echo -e "$fx" | xargs -i echo "\\'{}\\'" | xargs echo)"
}}

cmd LfTmuxNewTab ${{
    source "$HOME/.config/lf/lfcd.sh"
    tmux neww -a "lfcd.bak"
}}

# cmd setbg "$1"
# cmd bulkrename $vidir
#


# # Bindings
map <c-f> $lf -remote "send $id select '$(fd --follow --hidden --color=always | fzf --ansi)'"
# map <c-f> $lf -remote "send $id select '$(fd --follow --hidden | fzf )'"
# map J $lf -remote "send $id cd $(sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs | fzf)"

map y
map yy copy
map yp $echo -n "$f" | xclip -sel c
map yd $echo -n $(dirname "$f") | xclip -sel c
map yn $echo -n $(basename "$f") | xclip -sel c

map gh
map gg top

map d
map dd cut
map dD delete
map dt trash
map dc %du -sh $f

map xe $extract "$f"

# map C copyto
# map M moveto
# map zM moveto

# map a push :touch<space>
# map A push :mkdir<space>
map <c-n> push $func-alias-newFile<space>
# map <c-s-n> push $func-alias-newDirectory<space> # TODO: if ctrl shift map already a feature

# map <a-F> push $_zsh_func_ezel


map <c-r> reload

# map zz :FzfAutoJump
map <c-g> :FzfFormarks
map zz :FzfFormarks
map zf :FzfFormarksMark
map zM :FzfFormarksMv
map zC :FzfFormarksCp
map zt :LfTmuxNewTab
# map zl :zlLinkSymbolLink

map u
map uy clear
map uv unselect

map <enter> shell
map xx $$f

map o
map otw $tmux neww -c $PWD
map ote $tmux neww -c $PWD "lfcd"
map ots $tmux-sessionizer $PWD
map od $launch $PWD
map oo launchCmd $f
map os launchCmdDevour $f
map ott openType $f

map <c-o> $lf -remote "send $id cd \"$({ find '/media/metfoil16/Data/pets/linux/school' -type d & sed -e 's/#.*//' -e '/^$/d' -e 's/^\S*\s*//' ${XDG_CONFIG_HOME:-$HOME/.config}/shell/bm-dirs; } | fzf | sed 's|~|$HOME|')\""

map i
map ij imageMergeCmd

map A rename # at the very end
# map a push A<a-b> # after extention
map a push $create<space>
# map I push A<c-a> # after extention
map <f-2> push A # after extention
map I push A<a-b><a-b><a-f> # before extension

# map B bulkrename
# map X !$f
# map o &mimeopen $f
# map O $mimeopen --ask $f
# map O $mimeopen --ask $f
# map <c-h> set hidden!
# map c push A<c-u> # new rename
# map I push A<c-a> # at the very beginning
# map i push A<a-b><a-b><a-f> # before extention
# map b $setbg $f

# map <c-d> down
# map <c-u> up
map <c-e> push :!nvim<space>

map W $setsid -f $TERMINAL >/dev/null 2>&1

# Source Bookmarks
source "~/.config/shell/lfshortcuts"
---
2024-12-25 20:02:12: python references/deploy/video.py -c
configs/rtdetrv2/rtdetrv2_r18vd_120e_coco.yml -r output/checkpoint0119.pth -f
1211.mp4 -d cuda
---
2024-12-25 20:06:46: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os
import torch
import torch.nn as nn
import torchvision.transforms as T
from PIL import Image as PILImage
from PIL import ImageDraw
import sys

# Add RT-DETR path
sys.path.append('/path/to/rtdetrv2_pytorch')
from src.core import YAMLConfig

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        # RT-DETR setup
        self.setup_model()
        
        # Camera parameters
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        self.use_local_video = rospy.get_param('~use_local_video', False)
        self.video_path = rospy.get_param('~video_path', '')
        self.loop_video = rospy.get_param('~loop_video', True)
        
        self.image_pub = rospy.Publisher(f'/camera{self.camera_id}/image_raw', Image, queue_size=10)
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
        # Model transforms
        self.transforms = T.Compose([
            T.Resize((640, 640)),
            T.ToTensor(),
        ])
        
    def setup_model(self):
        """Initialize RT-DETR model"""
        try:
            config_path = rospy.get_param('~rtdetr_config')
            model_path = rospy.get_param('~rtdetr_model')
            self.device = rospy.get_param('~device', 'cpu')
            
            # Load configuration
            cfg = YAMLConfig(config_path, resume=model_path)
            checkpoint = torch.load(model_path, map_location='cpu')
            state = checkpoint['ema']['module'] if 'ema' in checkpoint else checkpoint['model']
            cfg.model.load_state_dict(state)
            
            # Setup model
            class Model(nn.Module):
                def __init__(self, cfg):
                    super().__init__()
                    self.model = cfg.model.deploy()
                    self.postprocessor = cfg.postprocessor.deploy()

                def forward(self, images, orig_target_sizes):
                    outputs = self.model(images)
                    outputs = self.postprocessor(outputs, orig_target_sizes)
                    return outputs

            self.model = Model(cfg).to(self.device)
            self.model.eval()
            rospy.loginfo("RT-DETR model loaded successfully")
            
        except Exception as e:
            rospy.logerr(f"Failed to load RT-DETR model: {str(e)}")
            raise

    def process_frame(self, frame):
        """Process frame through RT-DETR model"""
        try:
            # Convert to PIL
            pil_image = PILImage.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            w, h = pil_image.size
            orig_size = torch.tensor([w, h])[None].to(self.device)

            # Transform and predict
            img_tensor = self.transforms(pil_image)[None].to(self.device)
            
            with torch.no_grad():
                labels, boxes, scores = self.model(img_tensor, orig_size)

            # Draw predictions
            self.draw_predictions(pil_image, labels[0], boxes[0], scores[0])
            
            # Convert back to OpenCV format
            return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            
        except Exception as e:
            rospy.logerr(f"Error in frame processing: {str(e)}")
            return frame

    def draw_predictions(self, image, labels, boxes, scores, threshold=0.6):
        """Draw predictions on image"""
        draw = ImageDraw.Draw(image)
        
        valid_mask = scores > threshold
        valid_boxes = boxes[valid_mask]
        valid_labels = labels[valid_mask]
        valid_scores = scores[valid_mask]

        for box, label, score in zip(valid_boxes, valid_labels, valid_scores):
            draw.rectangle(list(box), outline='red', width=2)
            label_text = f"Class {label.item()}: {score.item():.2f}"
            draw.text((box[0], box[1] - 10), label_text, fill='red')

    # [Previous methods remain the same]
    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = self.cap.read()
                    
                    if not ret:
                        continue

                    # Process frame through RT-DETR
                    processed_frame = self.process_frame(frame)
                    
                    # Convert and publish
                    img_msg = self.numpy_to_image_msg(processed_frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-25 20:09:15: <param name="rtdetr_config" value="$(find gps_camera_monitoring)/config/rtdetrv2_r18vd_120e_coco.yml"/>
        <param name="rtdetr_model" value="$(find gps_camera_monitoring)/models/checkpoint0119.pth"/>
        <param name="device" value="cpu"/>
---
2024-12-25 20:40:46: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        # Parameter dasar
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        
        # Parameter baru untuk video lokal
        self.use_local_video = rospy.get_param('~use_local_video', False)  # Flag untuk switch mode
        self.video_path = rospy.get_param('~video_path', '')  # Path ke video lokal
        self.loop_video = rospy.get_param('~loop_video', True)  # Option untuk loop video
        
        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )
        
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message tanpa cv_bridge"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_local_video:
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Connected to local video: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)
                
                if not self.cap.isOpened():
                    gst_str = (
                        f"v4l2src device=/dev/video{self.camera_id} ! "
                        f"video/x-raw, width={self.width}, height={self.height} ! "
                        "videoconvert ! video/x-raw, format=BGR ! "
                        "appsink"
                    )
                    self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)
                    
            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_local_video else f'camera {self.camera_id}'}")
                    
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera {self.camera_id}'} connected successfully")
            return True
                
        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_local_video else f'camera {self.camera_id}'}: {str(e)}")
            return False
            
    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    # Handle video looping untuk local video
                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset video ke awal
                        ret, frame = self.cap.read()
                    
                    if not ret:
                        if self.use_local_video:
                            rospy.loginfo("End of video reached")
                            if not self.loop_video:
                                self.running = False
                            continue
                        else:
                            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                            continue
                    
                    # Resize frame jika diperlukan
                    if frame.shape[1] != self.width or frame.shape[0] != self.height:
                        frame = cv2.resize(frame, (self.width, self.height))
                    
                    # Convert dan publish
                    img_msg = self.numpy_to_image_msg(frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()
            
    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                
                rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start {'video' if self.use_local_video else f'camera'} node {self.camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error starting {'video' if self.use_local_video else f'camera'} node: {str(e)}")
            
    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-25 20:46:45: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os
import torch
import torch.nn as nn
import torchvision.transforms as T
from PIL import Image as PILImage
from PIL import ImageDraw
import sys
import time

# Add RT-DETR path
sys.path.append('/path/to/rtdetrv2_pytorch')
from src.core import YAMLConfig

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        
        # FPS calculation
        self.frame_count = 0
        self.start_time = time.time()
        self.fps = 0
        
        # RT-DETR setup
        self.setup_model()
        
        # Parameter dasar
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)
        
        # Parameter untuk video lokal
        self.use_local_video = rospy.get_param('~use_local_video', False)
        self.video_path = rospy.get_param('~video_path', '')
        self.loop_video = rospy.get_param('~loop_video', True)
        
        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )
        
        self.cap = None
        self.running = False
        self.lock = threading.Lock()
        
        # Model transforms
        self.transforms = T.Compose([
            T.Resize((640, 640)),
            T.ToTensor(),
        ])

        # Class mapping for detection
        self.class_mapping = {
            0: "AC", 1: "BC", 2: "BL", 3: "BS", 4: "CR", 5: "DP", 6: "EC",
            7: "JC", 8: "LS", 9: "LT", 10: "PA", 11: "PC", 12: "PH", 13: "RT",
            14: "SC", 15: "SL", 16: "SV", 17: "WR"
        }
        
    def setup_model(self):
        """Initialize RT-DETR model"""
        try:
            config_path = rospy.get_param('~rtdetr_config')
            model_path = rospy.get_param('~rtdetr_model')
            self.device = rospy.get_param('~device', 'cpu')
            
            # Load configuration
            cfg = YAMLConfig(config_path, resume=model_path)
            checkpoint = torch.load(model_path, map_location='cpu')
            state = checkpoint['ema']['module'] if 'ema' in checkpoint else checkpoint['model']
            cfg.model.load_state_dict(state)
            
            # Setup model
            class Model(nn.Module):
                def __init__(self, cfg):
                    super().__init__()
                    self.model = cfg.model.deploy()
                    self.postprocessor = cfg.postprocessor.deploy()

                def forward(self, images, orig_target_sizes):
                    outputs = self.model(images)
                    outputs = self.postprocessor(outputs, orig_target_sizes)
                    return outputs

            self.model = Model(cfg).to(self.device)
            self.model.eval()
            rospy.loginfo("RT-DETR model loaded successfully")
            
        except Exception as e:
            rospy.logerr(f"Failed to load RT-DETR model: {str(e)}")
            raise
    
    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_local_video:
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Connected to local video: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)
                
                if not self.cap.isOpened():
                    gst_str = (
                        f"v4l2src device=/dev/video{self.camera_id} ! "
                        f"video/x-raw, width={self.width}, height={self.height} ! "
                        "videoconvert ! video/x-raw, format=BGR ! "
                        "appsink"
                    )
                    self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)
                    
            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_local_video else f'camera {self.camera_id}'}")
                    
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera {self.camera_id}'} connected successfully")
            return True
                
        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_local_video else f'camera {self.camera_id}'}: {str(e)}")
            return False

    def process_frame(self, frame):
        """Process frame through RT-DETR model"""
        try:
            # Calculate FPS
            self.frame_count += 1
            current_time = time.time()
            elapsed_time = current_time - self.start_time
            if elapsed_time > 1.0:
                self.fps = self.frame_count / elapsed_time
                self.frame_count = 0
                self.start_time = current_time
            
            # Convert to PIL
            pil_image = PILImage.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            w, h = pil_image.size
            orig_size = torch.tensor([w, h])[None].to(self.device)

            # Transform and predict
            img_tensor = self.transforms(pil_image)[None].to(self.device)
            
            with torch.no_grad():
                labels, boxes, scores = self.model(img_tensor, orig_size)

            # Draw predictions
            self.draw_predictions(pil_image, labels[0], boxes[0], scores[0])
            
            # Convert back to OpenCV and add FPS
            processed_frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            cv2.putText(
                processed_frame,
                f"FPS: {self.fps:.2f}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 0),
                2,
                cv2.LINE_AA
            )
            return processed_frame
            
        except Exception as e:
            rospy.logerr(f"Error in frame processing: {str(e)}")
            return frame

    def draw_predictions(self, image, labels, boxes, scores, threshold=0.6):
        """Draw predictions on image"""
        draw = ImageDraw.Draw(image)
        
        valid_mask = scores > threshold
        valid_boxes = boxes[valid_mask]
        valid_labels = labels[valid_mask]
        valid_scores = scores[valid_mask]

        for box, label, score in zip(valid_boxes, valid_labels, valid_scores):
            # Draw bounding box
            draw.rectangle(list(box), outline='red', width=2)
            
            # Get class name
            class_id = label.item()
            class_name = self.class_mapping.get(class_id, "Unknown")
            
            # Draw label and score
            label_text = f"{class_name}: {score.item():.2f}"
            draw.text((box[0], box[1] - 10), label_text, fill='red')

    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue
                            
                    ret, frame = self.cap.read()
                    
                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = self.cap.read()
                    
                    if not ret:
                        if self.use_local_video:
                            rospy.loginfo("End of video reached")
                            if not self.loop_video:
                                self.running = False
                            continue
                        else:
                            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                            continue

                    # Process frame through RT-DETR
                    processed_frame = self.process_frame(frame)
                    
                    # Resize frame if needed
                    if processed_frame.shape[1] != self.width or processed_frame.shape[0] != self.height:
                        processed_frame = cv2.resize(processed_frame, (self.width, self.height))
                    
                    # Convert and publish
                    img_msg = self.numpy_to_image_msg(processed_frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)
                        
            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)
                
            rate = rospy.Rate(self.frame_rate)
            rate.sleep()
            
    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                
                rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start {'video' if self.use_local_video else f'camera'} node {self.camera_id}")
                
        except Exception as e:
            rospy.logerr(f"Error starting {'video' if self.use_local_video else f'camera'} node: {str(e)}")
            
    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-27 05:12:21: find -type f -exec grep -lr "InnoSys" {} \;
---
2024-12-27 05:12:38: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os
import torch
from torchvision import transforms
from PIL import Image as PILImage
import time
import sys

# Add Faster R-CNN model path
sys.path.append('/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch')
from model import FasterRCNN

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)

        # FPS calculation
        self.frame_count = 0
        self.start_time = time.time()
        self.fps = 0

        # Faster R-CNN setup
        self.setup_model()

        # Parameter dasar
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)

        # Parameter untuk video lokal
        self.use_local_video = rospy.get_param('~use_local_video', False)
        self.video_path = rospy.get_param('~video_path', '')
        self.loop_video = rospy.get_param('~loop_video', True)

        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )

        self.cap = None
        self.running = False
        self.lock = threading.Lock()

        # Class names for Faster R-CNN
        self.class_names = ["background", 'AC', 'BC', 'EC', 'JC', 'LT', 'SC', 'BS', 
                           'CR', 'DP', 'LS', 'PH', 'RT', 'SV', 'SL', 'PA', 'BL', 'WR', 'PC']

        # Transform for preprocessing
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def setup_model(self):
        """Initialize Faster R-CNN model"""
        try:
            # Model configuration from runVideo.py
            model_config = {
                "im_channels": 3,
                "aspect_ratios": [0.5, 1, 2],
                "scales": [128, 256, 512],
                "min_im_size": 600,
                "max_im_size": 1000,
                "backbone_out_channels": 512,
                "fc_inner_dim": 1024,
                "rpn_bg_threshold": 0.3,
                "rpn_fg_threshold": 0.7,
                "rpn_nms_threshold": 0.7,
                "rpn_train_prenms_topk": 12000,
                "rpn_test_prenms_topk": 6000,
                "rpn_train_topk": 2000,
                "rpn_test_topk": 300,
                "rpn_batch_size": 256,
                "rpn_pos_fraction": 0.5,
                "roi_iou_threshold": 0.5,
                "roi_low_bg_iou": 0.0,
                "roi_pool_size": 7,
                "roi_nms_threshold": 0.3,
                "roi_topk_detections": 100,
                "roi_score_threshold": 0.05,
                "roi_batch_size": 128,
                "roi_pos_fraction": 0.25,
            }

            # Load model
            self.model = FasterRCNN(model_config, num_classes=19)
            model_path = "/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"
            self.model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
            self.device = torch.device("cpu")
            self.model = self.model.to(self.device)
            self.model.eval()
            
            rospy.loginfo("Faster R-CNN model loaded successfully")

        except Exception as e:
            rospy.logerr(f"Failed to load Faster R-CNN model: {str(e)}")
            raise

    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_local_video:
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Connected to local video: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)

            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_local_video else f'camera {self.camera_id}'}")

            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            return True

        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_local_video else f'camera {self.camera_id}'}: {str(e)}")
            return False

    def process_frame(self, frame):
        """Process frame through Faster R-CNN model"""
        try:
            # Calculate FPS
            self.frame_count += 1
            current_time = time.time()
            elapsed_time = current_time - self.start_time
            if elapsed_time > 1.0:
                self.fps = self.frame_count / elapsed_time
                self.frame_count = 0
                self.start_time = current_time

            # Convert frame to PIL Image
            image = PILImage.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            
            # Transform image
            input_image = self.transform(image).unsqueeze(0).to(self.device)

            # Get predictions
            with torch.no_grad():
                _, frcnn_output = self.model(input_image)

            # Get detection results
            boxes = frcnn_output['boxes'].cpu().numpy()
            scores = frcnn_output['scores'].cpu().numpy()
            labels = frcnn_output['labels'].cpu().numpy()

            # Draw detections
            processed_frame = frame.copy()
            self.draw_detections(processed_frame, boxes, scores, labels)

            # Add FPS counter
            cv2.putText(
                processed_frame,
                f"FPS: {self.fps:.2f}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 255),
                2
            )

            return processed_frame

        except Exception as e:
            rospy.logerr(f"Error in frame processing: {str(e)}")
            return frame

    def draw_detections(self, frame, boxes, scores, labels, score_threshold=0.5):
        """Draw detection results on frame"""
        for box, score, label in zip(boxes, scores, labels):
            if score < score_threshold:
                continue

            x1, y1, x2, y2 = box.astype(int)
            class_name = self.class_names[label] if label < len(self.class_names) else f"class{label}"
            
            # Draw bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # Draw label and score
            label_text = f"{class_name}: {score:.2f}"
            cv2.putText(frame, label_text, (x1, y1 - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue

                    ret, frame = self.cap.read()

                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = self.cap.read()

                    if not ret:
                        if self.use_local_video:
                            rospy.loginfo("End of video reached")
                            if not self.loop_video:
                                self.running = False
                            continue
                        else:
                            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                            continue

                    # Process frame through Faster R-CNN
                    processed_frame = self.process_frame(frame)

                    # Convert and publish
                    img_msg = self.numpy_to_image_msg(processed_frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)

            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)

            rate = rospy.Rate(self.frame_rate)
            rate.sleep()

    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start {'video' if self.use_local_video else f'camera'} node {self.camera_id}")
        except Exception as e:
            rospy.logerr(f"Error starting {'video' if self.use_local_video else f'camera'} node: {str(e)}")

    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-27 05:21:41: <launch>
    <!-- Load parameter -->
    <rosparam command="load" file="$(find gps_camera_monitoring)/config/params.yaml" />
    
    <!-- GPS Node -->
    <node name="gps_node" pkg="gps_camera_monitoring" type="gps_node.py" output="screen">
        <param name="port" value="/dev/ttyTHS1"/>
        <param name="baudrate" value="9600"/>
    </node>

    <!-- Camera Node dengan parameter video lokal -->
    <node name="camera0_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
        <param name="camera_id" value="0"/>
        <param name="use_local_video" value="true"/>
        <param name="video_path" value="$(find gps_camera_monitoring)media/videos/yqq.mp4"/>
        <param name="loop_video" value="true"/>
        <!-- Hapus parameter RT-DETR dan tambahkan parameter Faster R-CNN -->
        <param name="model_path" value="/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"/>
        <param name="device" value="cpu"/>
    </node>

    <!-- Recorder Node -->
    <node name="recorder_node" pkg="gps_camera_monitoring" type="recorder_node.py" output="screen">
        <param name="save_path" value="$(env HOME)/recorded_data"/>
    </node>

    <!-- RQT GUI -->
    <node name="rqt_gui" pkg="rqt_gui" type="rqt_gui" output="screen"/>
</launch>
---
2024-12-27 05:21:50: gps:
  port: /dev/ttyTHS1
  baudrate: 9600
  update_rate: 1.0  # Hz
  frame_id: "gps"
  min_satellites: 4
  min_signal: 25

cameras:
  frame_rate: 30
  resolution:
    width: 640
    height: 480
  buffer_size: 1
  retry_interval: 1.0
  # Ganti konfigurasi RT-DETR dengan Faster R-CNN
  faster_rcnn:
    model_path: "/home/ris/gps_camera_ws/ML/train/FASTER_RCNN_F/FasterRCNN-PyTorch/model latih/faster_rcnn_road.pth"
    device: "cpu"
    score_threshold: 0.5
  local_video:
    enabled: false
    path: ""
    loop: true
  gstreamer:
    enable: true
    pipeline_template: "v4l2src device=/dev/video{} ! video/x-raw, width={}, height={} ! videoconvert ! video/x-raw, format=BGR ! appsink"

recorder:
  save_path: ~/recorded_data
  record_interval: 1.0
---
2024-12-27 06:33:58: https://drive.google.com/drive/u/1/folders/1BHHPV-mFED_gc5EouehWzUimds9RK4_q
---
2024-12-27 07:25:11: <!-- complete_system.launch -->
<launch>
    <!-- Load parameter -->
    <rosparam command="load" file="$(find gps_camera_monitoring)/config/params.yaml" />
    
    <!-- GPS Node -->
    <node name="gps_node" pkg="gps_camera_monitoring" type="gps_node.py" output="screen">
        <param name="port" value="/dev/ttyTHS1"/>
        <param name="baudrate" value="9600"/>
    </node>

    <!-- Camera Node -->
    <node name="camera0_node" pkg="gps_camera_monitoring" type="camera_node.py" output="screen" respawn="true">
        <param name="camera_id" value="0"/>
        <param name="use_local_video" value="true"/>
        <param name="video_path" value="$(find gps_camera_monitoring)/media/videos/yqq.mp4"/>
        <param name="loop_video" value="true"/>
        <param name="yolo_model" value="runs/detect/train5/weights/best.pt"/>
        <param name="device" value="cuda"/>
    </node>

    <!-- Recorder Node -->
    <node name="recorder_node" pkg="gps_camera_monitoring" type="recorder_node.py" output="screen">
        <param name="save_path" value="$(env HOME)/recorded_data"/>
    </node>

    <!-- RQT GUI -->
    <node name="rqt_gui" pkg="rqt_gui" type="rqt_gui" output="screen"/>
</launch>

# params.yaml
cameras:
  frame_rate: 30
  resolution:
    width: 640
    height: 480
  buffer_size: 1
  retry_interval: 1.0
  yolo:
    model: "runs/detect/train5/weights/best.pt"
    device: "cuda"
  local_video:
    enabled: false
    path: ""
    loop: true
  gstreamer:
    enable: true
    pipeline_template: "v4l2src device=/dev/video{} ! video/x-raw, width={}, height={} ! videoconvert ! video/x-raw, format=BGR ! appsink"
---
2024-12-27 07:25:17: #!/usr/bin/env python3

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
import threading
import os
import torch
from ultralytics import YOLO
import time

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)

        # FPS calculation
        self.frame_count = 0
        self.start_time = time.time()
        self.fps = 0

        # YOLO setup
        self.setup_model()

        # Parameter dasar
        self.camera_id = rospy.get_param('~camera_id', 0)
        self.frame_rate = rospy.get_param('~frame_rate', 30)
        self.width = rospy.get_param('~width', 640)
        self.height = rospy.get_param('~height', 480)

        # Parameter untuk video lokal
        self.use_local_video = rospy.get_param('~use_local_video', False)
        self.video_path = rospy.get_param('~video_path', '')
        self.loop_video = rospy.get_param('~loop_video', True)

        self.image_pub = rospy.Publisher(
            f'/camera{self.camera_id}/image_raw',
            Image,
            queue_size=10
        )

        self.cap = None
        self.running = False
        self.lock = threading.Lock()

    def setup_model(self):
        """Initialize YOLO model"""
        try:
            model_path = rospy.get_param('~yolo_model', 'runs/detect/train5/weights/best.pt')
            self.device = rospy.get_param('~device', 'cuda' if torch.cuda.is_available() else 'cpu')
            
            # Load YOLO model
            self.model = YOLO(model_path)
            self.model.to(self.device)
            
            rospy.loginfo(f"YOLO model loaded successfully on {self.device}")

        except Exception as e:
            rospy.logerr(f"Failed to load YOLO model: {str(e)}")
            raise

    def numpy_to_image_msg(self, frame):
        """Convert numpy array ke ROS image message"""
        msg = Image()
        msg.height = frame.shape[0]
        msg.width = frame.shape[1]
        msg.encoding = 'bgr8'
        msg.is_bigendian = False
        msg.step = frame.shape[1] * 3
        msg.data = frame.tobytes()
        return msg

    def connect_camera(self):
        try:
            if self.use_local_video:
                if not os.path.exists(self.video_path):
                    raise Exception(f"Video file not found: {self.video_path}")
                self.cap = cv2.VideoCapture(self.video_path)
                rospy.loginfo(f"Connected to local video: {self.video_path}")
            else:
                self.cap = cv2.VideoCapture(self.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                self.cap.set(cv2.CAP_PROP_FPS, self.frame_rate)

                if not self.cap.isOpened():
                    gst_str = (
                        f"v4l2src device=/dev/video{self.camera_id} ! "
                        f"video/x-raw, width={self.width}, height={self.height} ! "
                        "videoconvert ! video/x-raw, format=BGR ! "
                        "appsink"
                    )
                    self.cap = cv2.VideoCapture(gst_str, cv2.CAP_GSTREAMER)

            if not self.cap.isOpened():
                raise Exception(f"Could not open {'video file' if self.use_local_video else f'camera {self.camera_id}'}")

            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera {self.camera_id}'} connected successfully")
            return True

        except Exception as e:
            rospy.logerr(f"Failed to connect to {'video' if self.use_local_video else f'camera {self.camera_id}'}: {str(e)}")
            return False

    def process_frame(self, frame):
        """Process frame through YOLO model"""
        try:
            # Calculate FPS
            self.frame_count += 1
            current_time = time.time()
            elapsed_time = current_time - self.start_time
            if elapsed_time > 1.0:
                self.fps = self.frame_count / elapsed_time
                self.frame_count = 0
                self.start_time = current_time

            # Perform YOLO prediction
            results = self.model.predict(
                source=frame,
                device=self.device,
                verbose=False
            )

            # Get annotated frame
            processed_frame = results[0].plot()

            # Add FPS counter
            cv2.putText(
                processed_frame,
                f"FPS: {self.fps:.2f}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 0),
                2,
                cv2.LINE_AA
            )

            return processed_frame

        except Exception as e:
            rospy.logerr(f"Error in frame processing: {str(e)}")
            return frame

    def capture_and_publish(self):
        while not rospy.is_shutdown() and self.running:
            try:
                with self.lock:
                    if self.cap is None or not self.cap.isOpened():
                        if not self.connect_camera():
                            rospy.sleep(1)
                            continue

                    ret, frame = self.cap.read()

                    if self.use_local_video and not ret and self.loop_video:
                        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        ret, frame = self.cap.read()

                    if not ret:
                        if self.use_local_video:
                            rospy.loginfo("End of video reached")
                            if not self.loop_video:
                                self.running = False
                            continue
                        else:
                            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
                            continue

                    # Process frame through YOLO
                    processed_frame = self.process_frame(frame)

                    # Resize frame if needed
                    if processed_frame.shape[1] != self.width or processed_frame.shape[0] != self.height:
                        processed_frame = cv2.resize(processed_frame, (self.width, self.height))

                    # Convert and publish
                    img_msg = self.numpy_to_image_msg(processed_frame)
                    img_msg.header.stamp = rospy.Time.now()
                    img_msg.header.frame_id = f"camera_{self.camera_id}"
                    self.image_pub.publish(img_msg)

            except Exception as e:
                rospy.logerr(f"Error in capture loop: {str(e)}")
                rospy.sleep(1)

            rate = rospy.Rate(self.frame_rate)
            rate.sleep()

    def start(self):
        try:
            if self.connect_camera():
                self.running = True
                self.capture_thread = threading.Thread(target=self.capture_and_publish)
                self.capture_thread.daemon = True
                self.capture_thread.start()
                rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} started successfully")
                rospy.spin()
            else:
                rospy.logerr(f"Failed to start {'video' if self.use_local_video else f'camera'} node {self.camera_id}")

        except Exception as e:
            rospy.logerr(f"Error starting {'video' if self.use_local_video else f'camera'} node: {str(e)}")

    def stop(self):
        self.running = False
        if hasattr(self, 'capture_thread'):
            self.capture_thread.join()
        if self.cap:
            self.cap.release()
        rospy.loginfo(f"{'Video' if self.use_local_video else f'Camera'} node {self.camera_id} stopped")

if __name__ == '__main__':
    try:
        camera_node = CameraNode()
        camera_node.start()
    except rospy.ROSInterruptException:
        pass
    finally:
        if 'camera_node' in locals():
            camera_node.stop()
---
2024-12-27 07:48:41: import cv2
import time
from ultralytics import YOLO
import torch

# Pastikan CUDA tersedia
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Load model
model = YOLO('runs/detect/train5/weights/best.pt')  # Ganti dengan path ke model YOLO Anda
model.to(device)  # Pindahkan model ke GPU

# Load video
video_path = 'testvid/yqq.mp4'  # Path ke video input
output_path = 'testvid/output_video_jetson.mp4'  # Path untuk menyimpan video output

cap = cv2.VideoCapture(video_path)
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS),
                      (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))

# Buat window untuk display
cv2.namedWindow('YOLO Prediction', cv2.WINDOW_NORMAL)

frame_count = 0
total_time = 0
start_time = time.time()

try:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Mulai penghitungan waktu untuk frame ini
        frame_start_time = time.time()

        # Perform prediction (gunakan GPU jika tersedia)
        results = model.predict(source=frame, device=device, save=False, verbose=False)

        # Hitung waktu untuk frame ini
        elapsed_time = time.time() - frame_start_time
        total_time += elapsed_time
        frame_count += 1

        # Hitung FPS
        fps = frame_count / (time.time() - start_time)

        # Visualize results on the frame
        annotated_frame = results[0].plot()  # Menggambar bounding box pada frame

        # Tambahkan teks FPS ke frame
        cv2.putText(annotated_frame, f"FPS: {fps:.2f}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

        # Write the frame into the output file
        out.write(annotated_frame)

        # Display the frame
        cv2.imshow('YOLO Prediction', annotated_frame)

        # Break loop with 'q' key
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("Interrupted by user")
except Exception as e:
    print(f"An error occurred: {str(e)}")
finally:
    # Clean up
    cap.release()
    out.release()
    cv2.destroyAllWindows()
    
    # Print final statistics
    print(f"FPS rata-rata selama prediksi: {fps:.2f}")
    print(f"Video hasil prediksi disimpan di: {output_path}")
---
2024-12-30 08:46:46: import sys
import cv2
import time
import logging
import threading
import os
import requests
import json
import signal
from typing import Optional, Dict
from dataclasses import dataclass
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QLabel, 
                           QVBoxLayout, QHBoxLayout, QPushButton, QGridLayout,
                           QFrame, QSizePolicy, QMessageBox)
from PyQt5.QtCore import Qt, QTimer, QSize, QThread, pyqtSignal, QMutex, QMutexLocker,QObject
from PyQt5.QtGui import QImage, QPixmap, QFont, QPalette, QColor


from PyQt5.QtWidgets import (QDialog, QVBoxLayout, QHBoxLayout, QLabel, 
                           QPushButton, QProgressBar, QWidget, QFrame)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QUrl, QSize
from PyQt5.QtWebKitWidgets import QWebView
from PyQt5.QtWebKit import QWebSettings
from PyQt5.QtGui import QFont, QIcon
import logging


# Setup logging with proper configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vending_machine.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Hardware Configuration
try:
    import RPi.GPIO as GPIO
    HARDWARE_AVAILABLE = False
except ImportError:
    logger.warning("RPi.GPIO not available - running in simulation mode")
    HARDWARE_AVAILABLE = False


# Water volume configurations
@dataclass
class WaterVolume:
    name: str
    pulses: int
    display_name: str
    price: str

WATER_VOLUMES = {
    "100 ml": WaterVolume("100 ml", 108, "100 ml", "Rp. 3.000"),
    "350 ml": WaterVolume("350 ml", 378, "350 ml", "Rp. 5.000"),
    "600 ml": WaterVolume("600 ml", 670, "600 ml", "Rp. 7.000"),
    "1 Liter": WaterVolume("1 Liter", 1080, "1 Liter", "Rp. 15.000")
}

import json
from dataclasses import dataclass
import time, requests
from typing import Optional, Dict, Any
from requests.exceptions import RequestException

logger = logging.getLogger(__name__)

@dataclass
class APIConfig:
    base_url: str
    machine_id: str
    timeout: int
    retry_attempts: int
    retry_delay: int

@dataclass
class HardwareConfig:
    flow_sensor_pin: int
    motor_pin: int
    esp32_ip: str
    esp32_port: int

@dataclass
class AppConfig:
    video_path: str
    log_file: str
    log_level: str
    update_interval: int

class ConfigManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.config_path = 'config.json'
            self.api_config: Optional[APIConfig] = None
            self.hardware_config: Optional[HardwareConfig] = None
            self.app_config: Optional[AppConfig] = None
            self.load_config()
            self.initialized = True
    
    def load_config(self):
        """Load configuration from JSON file with fallback values"""
        try:
            if not os.path.exists(self.config_path):
                self._create_default_config()
            
            with open(self.config_path, 'r') as f:
                config = json.load(f)
            
            self.api_config = APIConfig(
                base_url=config['api']['base_url'],
                machine_id=config['api']['machine_id'],
                timeout=config['api']['timeout'],
                retry_attempts=config['api']['retry_attempts'],
                retry_delay=config['api']['retry_delay']
            )
            
            self.hardware_config = HardwareConfig(
                flow_sensor_pin=config['hardware']['flow_sensor_pin'],
                motor_pin=config['hardware']['motor_pin'],
                esp32_ip=config['hardware']['esp32_ip'],
                esp32_port=config['hardware']['esp32_port']
            )
            
            self.app_config = AppConfig(
                video_path=config['app']['video_path'],
                log_file=config['app']['log_file'],
                log_level=config['app']['log_level'],
                update_interval=config['app']['update_interval']
            )
            
            logger.info("Configuration loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading config: {e}")
            self._create_default_config()
    
    def _create_default_config(self):
        """Create default configuration file"""
        default_config = {
            'api': {
                'base_url': 'http://localhost:8000',
                'machine_id': 'VM001',
                'timeout': 5,
                'retry_attempts': 3,
                'retry_delay': 1
            },
            'hardware': {
                'flow_sensor_pin': 20,
                'motor_pin': 21,
                'esp32_ip': '192.168.137.82',
                'esp32_port': 80
            },
            'app': {
                'video_path': 'yqq.mkv',
                'log_file': 'vending_machine.log',
                'log_level': 'INFO',
                'update_interval': 2
            }
        }
        
        try:
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=4)
            logger.info("Created default configuration file")
            self.load_config()
        except Exception as e:
            logger.error(f"Error creating default config: {e}")
            raise

    def save_config(self):
        """Save current configuration to file"""
        config = {
            'api': {
                'base_url': self.api_config.base_url,
                'machine_id': self.api_config.machine_id,
                'timeout': self.api_config.timeout,
                'retry_attempts': self.api_config.retry_attempts,
                'retry_delay': self.api_config.retry_delay
            },
            'hardware': {
                'flow_sensor_pin': self.hardware_config.flow_sensor_pin,
                'motor_pin': self.hardware_config.motor_pin,
                'esp32_ip': self.hardware_config.esp32_ip,
                'esp32_port': self.hardware_config.esp32_port
            },
            'app': {
                'video_path': self.app_config.video_path,
                'log_file': self.app_config.log_file,
                'log_level': self.app_config.log_level,
                'update_interval': self.app_config.update_interval
            }
        }
        
        try:
            with open(self.config_path, 'w') as f:
                json.dump(config, f, indent=4)
            logger.info("Configuration saved successfully")
        except Exception as e:
            logger.error(f"Error saving config: {e}")
            raise


class APIClient:
    """Handle all API communications with backend"""
    
    def __init__(self):
        self.config = ConfigManager()
        self.session = requests.Session()
        self._setup_session()
    
    def _setup_session(self):
        """Configure requests session"""
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })
    
    def _make_request(self, method: str, endpoint: str, data: Optional[Dict] = None) -> Optional[Dict]:
        """
        Make HTTP request with retry mechanism and proper error handling
        """
        url = f"{self.config.api_config.base_url}/api/{endpoint}"
        retries = self.config.api_config.retry_attempts
        
        for attempt in range(retries):
            try:
                response = self.session.request(
                    method=method,
                    url=url,
                    json=data,
                    timeout=self.config.api_config.timeout
                )
                
                response.raise_for_status()
                return response.json()
                
            except RequestException as e:
                logger.warning(f"Request failed (attempt {attempt + 1}/{retries}): {e}")
                
                if attempt < retries - 1:
                    time.sleep(self.config.api_config.retry_delay)
                    continue
                    
                logger.error(f"Request failed after {retries} attempts: {e}")
                return None
                
            except Exception as e:
                logger.error(f"Unexpected error in API request: {e}")
                return None
    
    def record_quality(self, quality_data: Dict[str, float]) -> bool:
        """
        Record water quality data
        
        Args:
            quality_data: Dictionary containing tds_level, ph_level, and water_level
        
        Returns:
            bool: True if successful, False otherwise
        """
        endpoint = f"machines/{self.config.api_config.machine_id}/record_quality/"
        
        try:
            result = self._make_request('POST', endpoint, quality_data)
            return result is not None
        except Exception as e:
            logger.error(f"Error recording quality data: {e}")
            return False
    
    def record_sale(self, sale_data: Dict[str, Any]) -> bool:
        """
        Record sales data
        
        Args:
            sale_data: Dictionary containing volume and price
        
        Returns:
            bool: True if successful, False otherwise
        """
        endpoint = f"machines/{self.config.api_config.machine_id}/record_sale/"
        
        try:
            result = self._make_request('POST', endpoint, sale_data)
            return result is not None
        except Exception as e:
            logger.error(f"Error recording sale data: {e}")
            return False


from PyQt5.QtWidgets import (QDialog, QVBoxLayout, QHBoxLayout, QLabel, 
                           QPushButton, QProgressBar, QWidget, QFrame)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QUrl, QSize

from PyQt5.QtGui import QFont, QIcon
import logging

logger = logging.getLogger(__name__)

import midtransclient
import json
import logging
import uuid
from dataclasses import dataclass
from typing import Optional, Dict
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class PaymentConfig:
    server_key: str
    client_key: str
    merchant_id: str
    is_production: bool = False

class MidtransPayment:
    def __init__(self, config: PaymentConfig):
        self.config = config
        self.snap = midtransclient.Snap(
            is_production=config.is_production,
            server_key=config.server_key,
            client_key=config.client_key
        )

    def create_transaction(self, amount: int, item_details: dict) -> Optional[Dict]:
        """
        Create a Midtrans payment transaction
        
        Args:
            amount: Amount in IDR (integer)
            item_details: Dictionary containing item information
            
        Returns:
            Dictionary containing transaction token and redirect URL
        """
        try:
            order_id = f"WVM-{datetime.now().strftime('%Y%m%d')}-{uuid.uuid4().hex[:8]}"
            
            transaction = {
                "transaction_details": {
                    "order_id": order_id,
                    "gross_amount": amount
                },
                "item_details": [{
                    "id": item_details['id'],
                    "price": amount,
                    "quantity": 1,
                    "name": item_details['name'],
                    "merchant_name": "Water Vending Machine"
                }],
                 "enabled_payments": ["other_qris", "gopay", "DANA", "credit_card"],  # Hanya aktifkan QRIS
                    
                    # Opsional: Custom QRIS configuration
                    # "qris": {
                    #     "acquirer": "gopay"  # Atau bisa menggunakan acquirer lain yang didukung
                    # }
                # ,
                # "callbacks": {
                #     "finish": "http://localhost:8000/payment/finish",
                #     "error": "http://localhost:8000/payment/error",
                #     "cancel": "http://localhost:8000/payment/cancel"
                # }
            }


            # Tambahkan redirect_url
            # transaction["redirect_url"] = "http://localhost/payment/finish"  # Ubah sesuai URL Anda

            transaction_token = self.snap.create_transaction(transaction)
            
            return {
                "token": transaction_token["token"],
                "redirect_url": transaction_token["redirect_url"],
                "order_id": order_id
            }

        except Exception as e:
            logger.error(f"Error creating Midtrans transaction: {e}")
            return None

    def check_transaction_status(self, order_id: str) -> Optional[Dict]:
        """
        Check the status of a transaction
        
        Args:
            order_id: Order ID to check
            
        Returns:
            Dictionary containing transaction status and details
        """
        try:
            status_response = self.snap.transactions.status(order_id)
            return {
                "status": status_response["transaction_status"],
                "payment_type": status_response.get("payment_type", ""),
                "gross_amount": status_response.get("gross_amount", 0)
            }
        except Exception as e:
            logger.error(f"Error checking transaction status: {e}")
            return None

class PaymentManager:
    def __init__(self):
        self.config = self._load_payment_config()
        self.payment_client = MidtransPayment(self.config)
        
    def _load_payment_config(self) -> PaymentConfig:
        """Load Midtrans configuration from config file"""
        try:
            with open('payment_config.json', 'r') as f:
                config = json.load(f)
                return PaymentConfig(
                    server_key=config['server_key'],
                    client_key=config['client_key'],
                    merchant_id=config['merchant_id'],
                    is_production=config.get('is_production', False)
                )
        except Exception as e:
            logger.error(f"Error loading payment config: {e}")
            # Return development config for testing
            return PaymentConfig(
                server_key="SB-Mid-server-XXXXXX",
                client_key="SB-Mid-client-XXXXXX",
                merchant_id="M-XXXXXX",
                is_production=False
            )

from PyQt5.QtWidgets import (QDialog, QVBoxLayout, QHBoxLayout, QLabel, 
                           QPushButton, QWidget, QFrame)
from PyQt5.QtCore import Qt, QTimer, pyqtSignal, QUrl

import logging

logger = logging.getLogger(__name__)

class FullScreenPaymentDialog(QDialog):
    payment_completed = pyqtSignal(bool, str)  # success status, order_id
    
    def __init__(self, payment_manager, amount: int, item_details: dict, parent=None):
        super().__init__(parent)
        self.payment_manager = payment_manager
        self.amount = amount
        self.item_details = item_details
        self.transaction_data = None
        
        # Timer untuk cek status pembayaran
        self.status_check_timer = QTimer()
        self.status_check_timer.setInterval(3000)  # Cek setiap 3 detik
        
        # Timer untuk timeout (5 menit)
        self.timeout_duration = 5 * 60  # dalam detik
        # self.timeout_duration = 9  # dalam detik
        self.time_remaining = self.timeout_duration
        
        # Timer untuk countdown
        self.countdown_timer = QTimer()
        self.countdown_timer.setInterval(1000)  # setiap 1 detik
        self.countdown_timer.timeout.connect(self._update_countdown)
        
        self.initUI()
        
    def initUI(self):
        self.showFullScreen()
        
        self.setWindowFlags(Qt.FramelessWindowHint)
        
        # Main layout
        main_layout = QVBoxLayout(self)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # Header
        header = QFrame()
        header.setStyleSheet("""
            QFrame {
                background-color: #2C3E50;
                color: white;
                padding: 10px;
            }
        """)
        header.setFixedHeight(60)
        
        header_layout = QHBoxLayout(header)
        header_layout.setContentsMargins(15, 0, 15, 0)
        
        # Title
        title = QLabel("Payment Gateway")
        title.setStyleSheet("""
            color: white;
            font-size: 18px;
            font-weight: bold;
            font-family: 'Segoe UI', Arial;
        """)
        
        # Countdown label
        self.countdown_label = QLabel()
        self.countdown_label.setStyleSheet("""
            color: white;
            font-size: 16px;
            font-family: 'Segoe UI', Arial;
            padding: 0 20px;
        """)
        self._update_countdown()  # Set initial countdown display
        
        # Cancel button
        self.cancel_btn = QPushButton("Cancel")
        self.cancel_btn.setFixedSize(100, 35)
        self.cancel_btn.setStyleSheet("""
            QPushButton {
                background-color: #E74C3C;
                color: white;
                border: none;
                padding: 8px 15px;
                border-radius: 4px;
                font-size: 14px;
                font-weight: bold;
                font-family: 'Segoe UI', Arial;
            }
            QPushButton:hover {
                background-color: #C0392B;
            }
            QPushButton:pressed {
                background-color: #A93226;
            }
        """)
        self.cancel_btn.clicked.connect(self.handle_cancel)
        
        header_layout.addWidget(title)
        header_layout.addWidget(self.countdown_label, alignment=Qt.AlignCenter)
        header_layout.addWidget(self.cancel_btn)
        
        # Web view
        self.web_view = QWebView()
        
        main_layout.addWidget(header)
        main_layout.addWidget(self.web_view)
        
        # Start payment process
        self.start_payment()
    
    def start_payment(self):
        """Mulai proses pembayaran"""
        try:
            self.transaction_data = self.payment_manager.payment_client.create_transaction(
                self.amount,
                self.item_details
            )
            
            if self.transaction_data:
                # Load halaman pembayaran
                self.web_view.load(QUrl(self.transaction_data["redirect_url"]))
                
                # Mulai timer untuk status check
                self.status_check_timer.timeout.connect(self.check_payment_status)
                self.status_check_timer.start()
                
                # Mulai countdown
                self.time_remaining = self.timeout_duration
                self._update_countdown()  # Update display awal
                self.countdown_timer.start()
                
            else:
                self.handle_payment_failed("Failed to create transaction")
                
        except Exception as e:
            logger.error(f"Error starting payment: {e}")
            self.handle_payment_failed(str(e))
    
    def _update_countdown(self):
        """Update countdown display dan handle timeout"""
        if self.time_remaining > 0:
            minutes = self.time_remaining // 60
            seconds = self.time_remaining % 60
            self.countdown_label.setText(f"Time remaining: {minutes:02d}:{seconds:02d}")
            self.time_remaining -= 1
            logger.debug(f"Countdown updated: {minutes:02d}:{seconds:02d}")
        else:
            self.handle_timeout()
    
    def check_payment_status(self):
        """Check payment status periodically"""
        if not self.transaction_data:
            return
            
        try:
            status_data = self.payment_manager.payment_client.check_transaction_status(
                self.transaction_data["order_id"]
            )
            
            if status_data:
                status = status_data["status"]
                logger.info(f"Payment status: {status}")
                
                if status in ["settlement", "capture"]:
                    logger.info("Payment completed successfully")
                    self.countdown_timer.stop()
                    self.status_check_timer.stop()
                    self.payment_completed.emit(True, self.transaction_data["order_id"])
                    self.close()
                elif status in ["deny", "cancel", "expire", "failure"]:
                    logger.info(f"Payment failed with status: {status}")
                    self.handle_payment_failed(f"Payment {status}")
                    
        except Exception as e:
            if hasattr(self, '_last_status') and self._last_status == "capture":
                logger.info("Transaction completed with capture status")
                self.countdown_timer.stop()
                self.status_check_timer.stop()
                self.payment_completed.emit(True, self.transaction_data["order_id"])
                self.close()
            elif "404" in str(e):
                # Biarkan timer tetap berjalan, mungkin transaksi masih diproses
                logger.info("Transaction still processing")
            else:
                logger.error(f"Error checking transaction status: {e}")
    
    def handle_timeout(self):
        """Handle payment timeout"""
        logger.info("Payment timeout")
        self.countdown_timer.stop()
        self.status_check_timer.stop()
        
        # QMessageBox.warning(
        #     self,
        #     "Timeout",
        #     "Payment time has expired. Please try again."
        # )
        
        self.payment_completed.emit(False, self.transaction_data["order_id"] if self.transaction_data else "")
        self.close()
    
    def handle_payment_failed(self, error_msg: str):
        """Handle payment failure"""
        logger.error(f"Payment failed: {error_msg}")
        self.countdown_timer.stop()
        self.status_check_timer.stop()
        
        QMessageBox.warning(
            self,
            "Error",
            f"Payment failed: {error_msg}"
        )
        
        self.payment_completed.emit(False, "")
        self.close()
    
    def handle_cancel(self):
        """Handle manual cancellation"""
        logger.info("Payment cancelled by user")
        self.countdown_timer.stop()
        self.status_check_timer.stop()
        self.payment_completed.emit(False, self.transaction_data["order_id"] if self.transaction_data else "")
        self.close()
    
    def closeEvent(self, event):
        """Cleanup pada saat dialog ditutup"""
        logger.debug("Cleaning up payment dialog")
        self.countdown_timer.stop()
        self.status_check_timer.stop()
        event.accept()

class HardwareController:
    """Manages hardware interactions with proper error handling and simulation support."""
    

    def __init__(self):
        self.config = ConfigManager()
        self.is_simulated = not HARDWARE_AVAILABLE
        self._setup_gpio()
        
    def _setup_gpio(self):
        """Initialize GPIO with proper error handling."""
        if self.is_simulated:
            return
            
        try:
            GPIO.setmode(GPIO.BCM)
            GPIO.setup(self.config.hardware_config.flow_sensor_pin, GPIO.IN, pull_up_down=GPIO.PUD_UP)
            GPIO.setup(self.config.hardware_config.motor_pin, GPIO.OUT)
            GPIO.output(self.config.hardware_config.motor_pin, GPIO.LOW)
            logger.info("GPIO setup completed successfully")
        except Exception as e:
            logger.error(f"Failed to setup GPIO: {e}")
            self.is_simulated = True
    
    def start_motor(self):
        """Start the water pump motor."""
        if self.is_simulated:
            logger.info("Simulated motor start")
            return True
            
        try:
            GPIO.output(MOTOR_PIN, GPIO.HIGH)
            logger.info("Motor started")
            return True
        except Exception as e:
            logger.error(f"Failed to start motor: {e}")
            return False
    
    def stop_motor(self):
        """Stop the water pump motor."""
        if self.is_simulated:
            logger.info("Simulated motor stop")
            return
            
        try:
            GPIO.output(MOTOR_PIN, GPIO.LOW)
            logger.info("Motor stopped")
        except Exception as e:
            logger.error(f"Failed to stop motor: {e}")
    
    def cleanup(self):
        """Clean up GPIO resources."""
        if self.is_simulated:
            return
            
        try:
            GPIO.cleanup()
            logger.info("GPIO cleanup completed")
        except Exception as e:
            logger.error(f"GPIO cleanup failed: {e}")

class WaterController(QObject):
    """Controls water dispensing with flow sensor monitoring."""
    
    update_progress = pyqtSignal(int)
    filling_complete = pyqtSignal()
    error_occurred = pyqtSignal(str)

    def __init__(self):
        super().__init__()
        self.config = ConfigManager()
        self.hardware = HardwareController()
        self.api_client = APIClient()
        self.pulse_count = 0
        self.is_running = False
        self.target_pulses = 0
        self._lock = threading.Lock()
        
    def pulse_callback(self, channel):
        """Handle flow sensor pulse with thread safety."""
        if not self.is_running:
            return
            
        with self._lock:
            self.pulse_count += 1
            progress = min(100, int((self.pulse_count / self.target_pulses) * 100))
            self.update_progress.emit(progress)
            
            if self.pulse_count >= self.target_pulses:
                self.stop_filling()
    
    def start_filling(self, size: str) -> bool:
        """Start the water filling process for given size."""
        if size not in WATER_VOLUMES:
            self.error_occurred.emit("Invalid size selected")
            return False

        volume = WATER_VOLUMES[size]
        self.target_pulses = volume.pulses
        self.pulse_count = 0
        self.is_running = True
        
        logger.info(f"Starting filling process for {size} with target {self.target_pulses} pulses")
        
        # Start filling process in separate thread
        threading.Thread(target=self._filling_process, daemon=True).start()
        return True
    
    def _filling_process(self):
        """Handle the filling process with proper error handling."""
        try:
            if not self.hardware.start_motor():
                self.error_occurred.emit("Failed to start motor")
                return
                
            if not self.hardware.is_simulated:
                GPIO.add_event_detect(self.config.hardware_config.flow_sensor_pin, GPIO.FALLING, 
                                    callback=self.pulse_callback)
            else:
                # Simulate flow sensor pulses in simulation mode
                self._simulate_flow()
                
        except Exception as e:
            logger.error(f"Error in filling process: {e}")
            self.error_occurred.emit(f"Filling error: {str(e)}")
            self.stop_filling()
    
    def _simulate_flow(self):
        """Simulate flow sensor pulses when hardware is not available."""
        while self.is_running and self.pulse_count < self.target_pulses:
            time.sleep(0.01)  # Simulate 10 pulses per second
            self.pulse_callback(None)
    
    def stop_filling(self):
        """Stop the filling process safely and record sale."""
        self.is_running = False
        self.hardware.stop_motor()
        
        if not self.hardware.is_simulated:
            try:
                GPIO.remove_event_detect(self.config.hardware_config.flow_sensor_pin)
            except Exception as e:
                logger.warning(f"Failed to remove event detection: {e}")
        
        # Record sale if completed successfully
        if hasattr(self, 'current_volume'):
            try:
                sale_data = {
                    'volume': self.current_volume,
                    'price': self.current_price
                }
                self.api_client.record_sale(sale_data)
            except Exception as e:
                logger.error(f"Failed to record sale: {e}")
        
        self.filling_complete.emit()
        logger.info(f"Filling completed. Pulses: {self.pulse_count}/{self.target_pulses}")
    
    def cleanup(self):
        """Clean up resources."""
        self.stop_filling()
        self.hardware.cleanup()

class VideoThread(QThread):
    frame_ready = pyqtSignal(QImage)
    
    def __init__(self, video_path):
        super().__init__()
        self.video_path = video_path
        self.running = True
        self.mutex = QMutex()
        
    def run(self):
        try:
            cap = cv2.VideoCapture(self.video_path)
            if not cap.isOpened():
                logger.error("Failed to open video file")
                return
                
            while self.running:
                ret, frame = cap.read()
                if not ret:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset video
                    continue
                
                with QMutexLocker(self.mutex):
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    h, w, ch = rgb_frame.shape
                    bytes_per_line = ch * w
                    
                    image = QImage(rgb_frame.data.tobytes(), w, h, bytes_per_line, QImage.Format_RGB888)
                    image = image.copy()
                    
                    self.frame_ready.emit(image)
                    
                time.sleep(0.033)  # ~30 FPS
                
            cap.release()
            
        except Exception as e:
            logger.error(f"Error in video thread: {e}")
            
    def cleanup(self):
        """Clean up thread resources"""
        self.stop()
        # self.wait()
        if not self.wait(5000):  # Wait with 5 second timeout
            self.terminate()  # Force terminate if timeout
            self.wait()  # Wait for termination
    
    def stop(self):
        """Stop the thread safely"""
        with QMutexLocker(self.mutex):
            self.running = False

class SensorThread(QThread):
    sensor_updated = pyqtSignal(dict)
    
    def __init__(self, esp32_ip):
        super().__init__()
        self.config = ConfigManager()
        self.api_client = APIClient()
        self.esp32_ip = esp32_ip
        self.running = True
        self._last_successful_data = None
        

    def run(self):
        """Main thread loop with proper error handling"""
        while self.running:
            try:
                # Get sensor data from ESP32
                data = self._get_sensor_data()
                
                if data:
                    # Update last successful data
                    self._last_successful_data = data
                    
                    # Send to backend
                    self._send_to_backend(data)
                    
                    # Emit signal with new data
                    self.sensor_updated.emit(data)
                else:
                    # If failed to get new data, use last known good data
                    if self._last_successful_data:
                        self.sensor_updated.emit({
                            **self._last_successful_data,
                            'stale': True  # Indicate data is not fresh
                        })
                    else:
                        self.sensor_updated.emit({'error': True})
                        
            except Exception as e:
                logger.error(f"Error in sensor thread: {e}")
                self.sensor_updated.emit({'error': True})
            
            # Wait before next update
            time.sleep(self.config.app_config.update_interval)
    
    def _get_sensor_data(self) -> dict:
        """Get sensor data from ESP32 with proper error handling"""
        try:
            response = requests.get(
                f"http://{self.config.hardware_config.esp32_ip}:{self.config.hardware_config.esp32_port}/data",
                timeout=self.config.api_config.timeout
            )
            
            if response.status_code == 200:
                return response.json()
            
            logger.warning(f"Failed to get sensor data: HTTP {response.status_code}")
            return None
            
        except requests.exceptions.RequestException as e:
            logger.error(f"ESP32 connection error: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error getting sensor data: {e}")
            return None
    
    def _send_to_backend(self, data: dict) -> None:
        """Send sensor data to backend"""
        try:
            quality_data = {
                'tds_level': data.get('tds', 0),
                'ph_level': data.get('ph', 7),
                'water_level': data.get('water_level', 0)
            }
            
            success = self.api_client.record_quality(quality_data)
            if not success:
                logger.warning("Failed to send quality data to backend")
                
        except Exception as e:
            logger.error(f"Error sending data to backend: {e}")
    
    def cleanup(self):
        """Clean up thread resources"""
        self.stop()
        # self.wait()
        if not self.wait(5000):  # 5 second timeout
            self.terminate()
            self.wait()
    
    def stop(self):
        """Stop the thread safely"""
        self.running = False

class WaterButton(QPushButton):
    def __init__(self, size_text, price_text, image_path, parent=None):
        super().__init__(parent)
        self.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)
        self.setFixedSize(150, 150)
        self.setCheckable(True)
        
        layout = QVBoxLayout()
        layout.setSpacing(5)
        layout.setContentsMargins(10, 10, 10, 10)
        self.setLayout(layout)
        
        icon_container = QLabel()
        icon_container.setFixedSize(80, 80)
        icon_container.setStyleSheet("""
            background-color: white;
            border-radius: 10px;
            padding: 5px;
        """)
        
        if os.path.exists(image_path):
            pixmap = QPixmap(image_path)
            scaled_pixmap = pixmap.scaled(70, 70, Qt.KeepAspectRatio, Qt.SmoothTransformation)
            icon_container.setPixmap(scaled_pixmap)
        icon_container.setAlignment(Qt.AlignCenter)
        
        size_label = QLabel(f"{size_text}\n{price_text}")
        size_label.setAlignment(Qt.AlignCenter)
        size_label.setStyleSheet("""
            color: #2C3E50;
            font-size: 16px;
            font-weight: bold;
            font-family: 'Segoe UI', Arial;
        """)
        
        layout.addWidget(icon_container, alignment=Qt.AlignCenter)
        layout.addWidget(size_label, alignment=Qt.AlignCenter)
        
        self.setStyleSheet("""
            WaterButton {
                background-color: #F8F9FA;
                border-radius: 15px;
                border: 2px solid #E9ECEF;
            }
            WaterButton:checked {
                background-color: #4EA8DE;
                border: 2px solid #5390D9;
            }
            WaterButton:checked QLabel {
                color: white;
            }
            WaterButton:hover {
                background-color: #E9ECEF;
                border: 2px solid #DEE2E6;
            }
        """)

class MonitoringWidget(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.config = ConfigManager()
        self.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)
        self.setMinimumHeight(150)
        
        self.setup_ui()
        self.setup_sensor_thread()
        
        # Register for cleanup if parent is WaterSustainabilityApp
        if isinstance(parent, WaterSustainabilityApp):
            parent.register_cleanup(self)

    def setup_ui(self):
        layout = QVBoxLayout()
        layout.setSpacing(8)
        layout.setContentsMargins(8, 8, 8, 8)
        self.setLayout(layout)
        
        title = QLabel("Water Quality Monitoring")
        title.setStyleSheet("""
            font-size: 18px;
            font-weight: bold;
            color: #2C3E50;
            font-family: 'Segoe UI', Arial;
        """)
        title.setAlignment(Qt.AlignCenter)
        
        monitor_container = QWidget()
        monitor_container.setStyleSheet("""
            background-color: #F8F9FA;
            border-radius: 15px;
            border: 2px solid #E9ECEF;
            padding: 10px;
        """)
        
        monitor_layout = QHBoxLayout(monitor_container)
        monitor_layout.setContentsMargins(8, 4, 8, 4)
        
        self.ph_widget = self.create_monitor_display("pH Value")
        self.tds_widget = self.create_monitor_display("TDS Value")
        
        monitor_layout.addWidget(self.ph_widget)
        monitor_layout.addWidget(self.tds_widget)
        
        layout.addWidget(title)
        layout.addWidget(monitor_container)

    def setup_sensor_thread(self):
        try:
            self.sensor_thread = SensorThread(self.config.hardware_config.esp32_ip)
            self.sensor_thread.sensor_updated.connect(self.update_sensor_display)
            self.sensor_thread.start()
        except Exception as e:
            logger.error(f"Failed to setup sensor thread: {e}")
            self.update_sensor_display({'error': True})

    def create_monitor_display(self, title):
        widget = QWidget()
        layout = QVBoxLayout(widget)
        layout.setSpacing(4)
        
        title_label = QLabel(title)
        title_label.setStyleSheet("""
            font-size: 14px;
            font-weight: bold;
            color: #2C3E50;
            padding: 2px;
        """)
        title_label.setAlignment(Qt.AlignCenter)
        
        value_label = QLabel("Not Connected")
        value_label.setStyleSheet("""
            background-color: white;
            padding: 8px;
            border-radius: 8px;
            font-size: 14px;
            font-weight: bold;
border: 1px solid #E9ECEF;
        """)
        value_label.setAlignment(Qt.AlignCenter)
        value_label.setWordWrap(True)
        
        layout.addWidget(title_label)
        layout.addWidget(value_label)
        
        # Store the value label reference
        if title == "pH Value":
            self.ph_value = value_label
        else:
            self.tds_value = value_label
        
        return widget

    def update_sensor_display(self, data):
        """Update display with sensor data"""
        try:
            if hasattr(self, 'ph_value') and hasattr(self, 'tds_value'):
                if 'error' in data:
                    self.ph_value.setText("Not Connected")
                    self.tds_value.setText("Not Connected")
                else:
                    ph_value = data.get('ph', 'Error')
                    tds_value = data.get('tds', 'Error')
                    self.ph_value.setText(f"{ph_value}")
                    self.tds_value.setText(f"{tds_value}")
        except Exception as e:
            logger.error(f"Error updating sensor display: {e}")
    
    def cleanup(self):
        """Clean up widget resources"""
        if hasattr(self, 'sensor_thread'):
            logger.debug("Cleaning up sensor thread")
            self.sensor_thread.cleanup()

class MachineWidget(QWidget):
    filling_completed = pyqtSignal()

    def __init__(self, parent=None):
        super().__init__(parent)
        self.main_window = parent
        self.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)
        self.setMinimumHeight(120)
        
        # Initialize water controller
        self.water_controller = WaterController()
        self.water_controller.update_progress.connect(self.update_progress)
        self.water_controller.filling_complete.connect(self.complete_filling)
        self.water_controller.error_occurred.connect(self.handle_error)
        
        # Initialize UI state
        self.progress = 0
        self.is_filling = False
        self.progress_segments = 18
        
        self.setup_layout()
        
        # Register for cleanup if parent is WaterSustainabilityApp
        if isinstance(parent, WaterSustainabilityApp):
            parent.register_cleanup(self)

    def handle_start_button_click(self):
        """Handle start button click"""
        if not self.is_filling:
            # Disable button during payment process
            self.start_button.setEnabled(False)
            
            # Start payment process
            if hasattr(self.main_window, 'start_payment_process'):
                print("Starting payment process")
                self.main_window.start_payment_process()
            else:
                print("Parent does not have start_payment_process method")

    def setup_layout(self):
        layout = QVBoxLayout()
        layout.setSpacing(10)
        layout.setContentsMargins(10, 10, 10, 10)
        self.setLayout(layout)

        layout.addWidget(self.create_progress_container())
        layout.addWidget(self.create_machine_display())

    def create_progress_container(self):
        progress_container = QWidget()
        progress_container.setStyleSheet("""
            background-color: #2C3E50;
            border-radius: 15px;
            padding: 10px;
        """)
        
        progress_layout = QHBoxLayout(progress_container)
        progress_layout.setContentsMargins(10, 5, 10, 5)
        
        self.progress_indicator = QLabel("█")
        self.progress_indicator.setStyleSheet("color: #2ECC71; font-size: 24px;")
        
        self.progress_bar = QLabel("▬" * self.progress_segments)
        self.progress_bar.setStyleSheet("color: white; font-size: 16px;")
        
        progress_layout.addWidget(self.progress_indicator)
        progress_layout.addWidget(self.progress_bar, 1)
        
        return progress_container

    def create_machine_display(self):
        machine_display = QWidget()
        machine_display.setStyleSheet("""
            background-color: #2C3E50;
            border-radius: 15px;
            padding: 10px;
        """)
        
        machine_layout = QVBoxLayout(machine_display)
        machine_layout.setSpacing(5)
        machine_layout.setContentsMargins(10, 5, 10, 5)
        
        self.machine_image = QLabel()
        self.machine_image.setFixedSize(80, 80)
        self.machine_image.setAlignment(Qt.AlignCenter)
        self.machine_image.setStyleSheet("background: transparent;")
        
        if os.path.exists("5.png"):
            machine_pixmap = QPixmap("5.png")
            scaled_machine = machine_pixmap.scaled(70, 70, Qt.KeepAspectRatio, Qt.SmoothTransformation)
            self.machine_image.setPixmap(scaled_machine)
        
        self.start_button = QPushButton("Start Filling")
        self.start_button.setFixedSize(150, 40)
        self.start_button.setEnabled(False)
        self.start_button.setStyleSheet("""
            QPushButton {
                background-color: #2ECC71;
                border-radius: 20px;
                color: white;
                font-size: 16px;
                font-weight: bold;
                font-family: 'Segoe UI', Arial;
                border: none;
            }
            QPushButton:disabled {
                background-color: #95A5A6;
            }
            QPushButton:hover:!disabled {
                background-color: #27AE60;
            }
        """)
        
        # self.start_button.clicked.connect(self.start_filling_animation)
        self.start_button.clicked.connect(self.handle_start_button_click)
    
        
        machine_layout.addWidget(self.machine_image, alignment=Qt.AlignCenter)
        machine_layout.addWidget(self.start_button, alignment=Qt.AlignCenter)
        
        return machine_display

    def start_filling_animation(self):
        """Start the water filling process"""
        if not self.is_filling and hasattr(self, 'selected_size'):
            success = self.water_controller.start_filling(self.selected_size)
            if success:
                self.is_filling = True
                self.progress = 0
                self.start_button.setEnabled(False)
                self.start_button.setText("Filling...")
                self.progress_indicator.setStyleSheet("color: #E74C3C; font-size: 24px;")

    def update_progress(self, progress):
        """Update progress bar visualization"""
        try:
            self.progress = progress
            filled = "█" * (progress * self.progress_segments // 100)
            empty = "▬" * (self.progress_segments - (progress * self.progress_segments // 100))
            self.progress_bar.setText(filled + empty)
        except Exception as e:
            logger.error(f"Error updating progress: {e}")

    def complete_filling(self):
        """Handle completion of filling process"""
        try:
            self.is_filling = False
            self.start_button.setEnabled(True)
            self.start_button.setText("Start Filling")
            self.progress_indicator.setStyleSheet("color: #2ECC71; font-size: 24px;")
            self.filling_completed.emit()
        except Exception as e:
            logger.error(f"Error completing filling: {e}")

    def handle_error(self, error_msg: str):
        """Handle errors during filling process"""
        logger.error(f"Filling error: {error_msg}")
        self.complete_filling()
        # You might want to add UI feedback for errors here

    def cleanup(self):
        """Clean up widget resources"""
        if hasattr(self, 'water_controller'):
            logger.debug("Cleaning up water controller")
            self.water_controller.cleanup()

class WaterSustainabilityApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self._cleanup_handlers = []
        self.config = ConfigManager()
        self.payment_manager = PaymentManager()
        self.initUI()

    def register_cleanup(self, handler):
        """Register objects that need cleanup"""
        self._cleanup_handlers.append(handler)
        logger.debug(f"Registered cleanup handler: {handler.__class__.__name__}")

    def initUI(self):
        self.setWindowTitle("Innovative Aqua Solution")
        self.setStyleSheet("background-color: #E3F2FD;")
        
        screen = QApplication.desktop().screenGeometry()
        width = int(screen.width() * 0.8)
        height = int(screen.height() * 0.8)
        self.setMinimumSize(1366, 768)
        self.resize(width, height)
        
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        main_layout = QVBoxLayout(main_widget)
        main_layout.setContentsMargins(20, 20, 20, 20)
        main_layout.setSpacing(20)
        
        content = QWidget()
        content.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        content_layout = QHBoxLayout(content)
        content_layout.setSpacing(20)
        
        # Left panel setup
        left_panel = QWidget()
        left_panel.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        left_layout = QVBoxLayout(left_panel)
        left_layout.setSpacing(20)
        
        # Video container
        video_container = self.create_video_container()
        left_layout.addWidget(video_container, 7)
        
        # Monitoring widget
        monitoring_widget = MonitoringWidget(self)
        left_layout.addWidget(monitoring_widget, 3)
        
        # Right panel setup
        right_panel = self.create_right_panel()
        
        content_layout.addWidget(left_panel, 7)
        content_layout.addWidget(right_panel, 0)
        
        main_layout.addWidget(content)
        
        self.setup_video()

    def start_payment_process(self):
        """Start the payment process when Start Filling is clicked"""
        if not hasattr(self.machine_widget, 'selected_size'):
            return
            
        size = self.machine_widget.selected_size
        volume = WATER_VOLUMES.get(size)
        if not volume:
            return
            
        # Convert price string to integer (remove "Rp. " and ".")
        price_str = volume.price.replace("Rp. ", "").replace(".", "")
        amount = int(price_str)
        
        # Prepare item details
        item_details = {
            'id': f"WATER_{size.replace(' ', '')}",
            'name': f"Water {volume.display_name}"
        }
        
        try:
            payment_dialog = FullScreenPaymentDialog(
                self.payment_manager, 
                amount,
                item_details,
                self
            )
            payment_dialog.payment_completed.connect(self.handle_payment_result)
            payment_dialog.exec_()
        except Exception as e:
            logger.error(f"Error starting payment: {e}")
            QMessageBox.warning(
                self,
                "Error",
                "Failed to start payment process. Please try again."
            )

    def handle_payment_result(self, success: bool, order_id: str):
        """Handle payment completion result"""
        if success:
            logger.info(f"Payment successful for order {order_id}")
            self.machine_widget.start_filling_animation()
        else:
            logger.warning(f"Payment failed or cancelled for order {order_id}")
            self.machine_widget.start_button.setEnabled(True)

    def create_video_container(self):
        video_container = QWidget()
        video_container.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        video_container.setStyleSheet("""
            background-color: #F8F9FA;
            border-radius: 20px;
            border: 2px solid #E9ECEF;
            padding: 10px;
        """)
        
        video_layout = QVBoxLayout(video_container)
        
        self.video_label = QLabel()
        self.video_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self.video_label.setMinimumSize(640, 480)
        self.video_label.setMaximumSize(1280, 720)
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setStyleSheet("""
            QLabel {
                background-color: #E9ECEF;
                border-radius: 10px;
            }
        """)
        
        video_layout.addWidget(self.video_label)
        return video_container

    def create_right_panel(self):
        right_panel = QWidget()
        right_panel.setFixedWidth(350)
        right_layout = QVBoxLayout(right_panel)
        right_layout.setSpacing(20)
        
        # Water selection section
        selection_widget = QWidget()
        selection_widget.setStyleSheet("""
            background-color: #F8F9FA;
            border-radius: 20px;
            border: 2px solid #E9ECEF;
            padding: 20px;
        """)
        
        selection_layout = QVBoxLayout(selection_widget)
        
        selection_title = QLabel("Select Water Size")
        selection_title.setStyleSheet("""
            font-size: 24px;
            font-weight: bold;
            color: #2C3E50;
            font-family: 'Segoe UI', Arial;
        """)
        selection_title.setAlignment(Qt.AlignCenter)
        
        buttons_grid = QGridLayout()
        buttons_grid.setSpacing(10)
        
        self.size_buttons = []
        button_positions = [
            ("100 ml", "1.png", 0, 0),
            ("350 ml", "2.png", 0, 1),
            ("600 ml", "3.png", 1, 0),
            ("1 Liter", "4.png", 1, 1)
        ]
        
        for size, image_path, row, col in button_positions:
            volume = WATER_VOLUMES.get(size)
            if volume and os.path.exists(image_path):
                btn = WaterButton(volume.name, volume.price, image_path)
                btn.clicked.connect(lambda checked, s=size: self.on_size_selected(s))
                buttons_grid.addWidget(btn, row, col)
                self.size_buttons.append(btn)
        
        selection_layout.addWidget(selection_title)
        selection_layout.addLayout(buttons_grid)
        selection_layout.addStretch()
        
        # Machine widget
        self.machine_widget = MachineWidget(self)
        print(f"Created MachineWidget with parent: {type(self)}")  # Debug print
        
        
        right_layout.addWidget(selection_widget)
        right_layout.addWidget(self.machine_widget)
        right_layout.addStretch()
        
        return right_panel

    def setup_video(self):
        """Set up video thread with proper cleanup registration"""
        try:
            video_path = self.config.app_config.video_path
            if not os.path.exists(video_path):
                self.video_label.setText("Video not found")
                return
                
            self.video_thread = VideoThread(video_path)
            self.video_thread.frame_ready.connect(self.update_video_frame, Qt.QueuedConnection)
            self.register_cleanup(self.video_thread)
            self.video_thread.start()
            
        except Exception as e:
            logger.error(f"Error setting up video: {e}")
            self.video_label.setText(f"Error: {str(e)}")

    def update_video_frame(self, image):
        """Update video frame with proper error handling"""
        try:
            if not self.video_label or image.isNull():
                return
                
            label_size = self.video_label.size()
            if not label_size.isValid():
                return
                
            scaled_pixmap = QPixmap.fromImage(image).scaled(
                label_size,
                Qt.KeepAspectRatio,
                Qt.SmoothTransformation
            )
            
            if not scaled_pixmap.isNull():
                self.video_label.setPixmap(scaled_pixmap)
                
        except Exception as e:
            logger.error(f"Error updating video frame: {e}")

    def on_size_selected(self, size):
        """Handle water size selection"""
        try:
            clicked_button = self.sender()
            for btn in self.size_buttons:
                if btn != clicked_button:
                    btn.setChecked(False)
            
            self.machine_widget.selected_size = size if clicked_button.isChecked() else None
            self.machine_widget.start_button.setEnabled(
                self.machine_widget.selected_size is not None
            )
        except Exception as e:
            logger.error(f"Error in size selection: {e}")

    def resizeEvent(self, event):
        """Handle window resize events"""
        try:
            super().resizeEvent(event)
            if hasattr(self, 'video_label'):
                new_width = min(1280, int(self.width() * 0.6))
                new_height = min(720, int(self.height() * 0.6))
                self.video_label.setMinimumSize(new_width, new_height)
        except Exception as e:
            logger.error(f"Error in resize event: {e}")

    def closeEvent(self, event):
        """Handle application shutdown with proper cleanup"""
        logger.info("Application shutdown initiated")
        
        try:


            # Set timeout for cleanup
            cleanup_timer = QTimer()
            cleanup_timer.setSingleShot(True)
            cleanup_timer.timeout.connect(lambda: self.force_cleanup())
            cleanup_timer.start(10000)  # 10 second timeout

            # Stop video thread if exists
            if hasattr(self, 'video_thread'):
                logger.debug("Stopping video thread")
                self.video_thread.cleanup()
            
            # Cleanup machine widget
            if hasattr(self, 'machine_widget'):
                logger.debug("Cleaning up machine widget")
                self.machine_widget.cleanup()
            
            # Run all registered cleanup handlers
            for handler in self._cleanup_handlers:
                try:
                    logger.debug(f"Running cleanup for: {handler.__class__.__name__}")
                    handler.cleanup()
                except Exception as e:
                    logger.error(f"Error during cleanup of {handler.__class__.__name__}: {e}")
            
            logger.info("Application cleanup completed successfully")
            cleanup_timer.stop()
            event.accept()
            
        except Exception as e:
            logger.error(f"Error during application shutdown: {e}")
            self.force_cleanup()
            event.accept()  # Accept the close event even if cleanup fails
            
        finally:
            # Final cleanup attempt for GPIO
            if HARDWARE_AVAILABLE:
                try:
                    GPIO.cleanup()
                    logger.info("GPIO cleanup completed")
                except Exception as e:
                    logger.error(f"Final GPIO cleanup failed: {e}")

    def force_cleanup(self):
        """Force cleanup all resources"""
        logger.warning("Forcing application cleanup")
        
        # Force stop all threads
        if hasattr(self, 'video_thread'):
            self.video_thread.terminate()
            self.video_thread.wait()
        
        if hasattr(self, 'sensor_thread'):
            self.sensor_thread.terminate()
            self.sensor_thread.wait()
        
        # Final GPIO cleanup
        if HARDWARE_AVAILABLE:
            try:
                GPIO.cleanup()
            except:
                pass

def create_app():
    """Create and initialize the application with proper error handling"""
    try:
        # Set up application-wide exception handling
        sys.excepthook = handle_exception
        
        app = QApplication(sys.argv)
        signal.signal(signal.SIGINT, lambda sig, frame: app.quit())
        
        # Set application-wide font
        app.setFont(QFont('Segoe UI', 10))
        
        # Create and show main window
        window = WaterSustainabilityApp()
        window.show()
        #window.showFullScreen()
        
        return app.exec_()
    
    except Exception as e:
        logger.critical(f"Failed to create application: {e}")
        return 1

def handle_exception(exc_type, exc_value, exc_traceback):
    """Handle uncaught exceptions"""
    if issubclass(exc_type, KeyboardInterrupt):
        # Handle keyboard interrupt specially
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
        
    logger.critical("Uncaught exception:", exc_info=(exc_type, exc_value, exc_traceback))

if __name__ == '__main__':
    try:
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('vending_machine.log'),
                logging.StreamHandler()
            ]
        )
        
        # Start application
        sys.exit(create_app())
        
    except KeyboardInterrupt:
        logger.info("Application terminated by user")
        if HARDWARE_AVAILABLE:
            GPIO.cleanup()
        sys.exit(0)
        
    except Exception as e:
        logger.critical(f"Critical application error: {e}")
        if HARDWARE_AVAILABLE:
            GPIO.cleanup()
---
2024-12-30 08:53:56: 2024-12-30 07:52:34,536 - __main__ - ERROR - ESP32 connection error: HTTPConnectionPool(host='192.168.137.82', port=80): Max retries exceeded with url: /data (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0xa05ffa50>: Failed to establish a new connection: [Errno 113] No route to host'))
---
2024-12-30 08:54:19: 2024-12-30 07:52:36,552 - __main__ - ERROR - Error checking transaction status: Midtrans API is returning API error. API status code: `404`. API response: `{"status_code":"404","status_message":"Transaction doesn't exist.","id":"8797f2e7-ab26-4af1-a8fa-6cbac9ec4d5e"}
---
2024-12-30 09:01:05: 2024-12-30 08:00:40,532 - __main__ - ERROR - Error checking transaction status: Midtrans API is returning API error. API status code: `404`. API response: `{"status_code":"404","status_message":"Transaction doesn't exist.","id":"e73ed594-d7a5-4ab7-8c19-5e61e275577c"}`
---
2024-12-30 09:13:19: qt5ct: D-Bus system tray: no
2024-12-30 08:12:53,503 - __main__ - INFO - Received token response: {'token': '830ac4a8-5413-4837-b547-2e2332515d84', 'redirect_url': 'https://app.sandbox.midtrans.com/snap/v4/redirection/830ac4a8-5413-4837-b547-2e2332515d84'}
2024-12-30 08:12:53,562 - __main__ - INFO - Loading payment URL: https://app.sandbox.midtrans.com/snap/v4/redirection/830ac4a8-5413-4837-b547-2e2332515d84
2024-12-30 08:12:54,744 - __main__ - ERROR - ESP32 connection error: HTTPConnectionPool(host='192.168.137.82', port=80): Max retries exceeded with url: /data (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0xa05fc8d0>: Failed to establish a new connection: [Errno 113] No route to host'))
2024-12-30 08:12:56,617 - __main__ - ERROR - Error checking transaction status: Midtrans API is returning API error. API status code: `404`. API response: `{"status_code":"404","status_message":"Transaction doesn't exist.","id":"ecaa9811-b3fd-425e-a0e1-e10332c86c63"}`
2024-12-30 08:12:57,880 - __main__ - ERROR - ESP32 connection error: HTTPConnectionPool(host='192.168.137.82', port=80):
---
2024-12-30 10:39:35: 'https://api.sandbox.midtrans.com/v2/qris/f3d92898-aab0-4732-b4b7-3f70fdfbafda/qr-code'}], 'acquirer': 'gopay', 'q
---
